{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd07ec3019a7cb8f5739f8b27d55fc304c51580b268fd2b34191f81e31e11772118",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import search\n",
    "import codecs\n",
    "import sys\n",
    "import string\n",
    "import math\n",
    "import heapq\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "prefixes = []\n",
    "postfixes = []\n",
    "verb_roots = []\n",
    "common_words = []\n",
    "plural_singular = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#######################################importing\n",
    "def read_dataset(path, name):\n",
    "    df = pd.read_excel(path + name)\n",
    "    return df\n",
    "\n",
    "def read_files():\n",
    "    path = 'files/'\n",
    "\n",
    "    #stop_words\n",
    "    name = 'stop_words.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        stop_words.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #prefixes\n",
    "    name = 'prefixes.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        prefixes.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #postfixes\n",
    "    name = 'postfixes.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        postfixes.append(line)\n",
    "    f.close()\n",
    "\n",
    "    # verb_roots\n",
    "    name = 'verb.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    count = 0\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        line = line.split('-')\n",
    "        verb_roots.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #common_words\n",
    "    name = 'common_words.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        common_words.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #plural_singular\n",
    "    name = 'plural_singular.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    count = 0\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        line = line.split('-')\n",
    "        plural_singular.append(line)\n",
    "    f.close()\n",
    "\n",
    "#######################################Normalization\n",
    "def delete_punctuations(doc):\n",
    "    punctuations = '؟،:؛!»«()[]*,{.}@!?،؛<>#$&!~\"\\|-_+'\n",
    "    edited_doc = doc.translate(str.maketrans('', '', punctuations))\n",
    "    edited_doc = edited_doc.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return edited_doc\n",
    "\n",
    "def delete_stopWords(doc):\n",
    "    edited_doc = doc\n",
    "    for s in stop_words:\n",
    "        my_regex = r\"\\b\"+s+r\"\\b\"\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def delete_highFrequencyWords(inverted_indexes):\n",
    "    for i in inverted_indexes:\n",
    "        if len(i[1])/len(df) > 0.8:\n",
    "            inverted_indexes.remove(i)\n",
    "    return inverted_indexes\n",
    "\n",
    "def delete_postfixes(doc):\n",
    "    edited_doc = doc\n",
    "    for p in postfixes:\n",
    "        my_regex = p + r\"\\b\"\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def delete_prefixes(doc):\n",
    "    edited_doc = doc\n",
    "    for p in postfixes:\n",
    "        my_regex = r\"\\b\" + p\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def replaceWithRoot(tokens): \n",
    "    for i in range(0, len(tokens)):\n",
    "        # print(verb_roots)\n",
    "        for root in verb_roots:\n",
    "            if search(root[0], tokens[i]) or search(root[1], tokens[i]):\n",
    "                common = False\n",
    "                for c in common_words:\n",
    "                    if c == tokens[i]:\n",
    "                        common = True\n",
    "                        # print(\"common\")\n",
    "                        break\n",
    "                if common == False:\n",
    "                    my_regex = r\"\\b^\"+root[1]+r\"\\.*\"\n",
    "                    x = re.findall(my_regex, tokens[i])\n",
    " \n",
    "                    if len(x) == 0:  \n",
    "                        my_regex = r\"\\b...*\"+root[1]+r\"\\b\"\n",
    "                        x1 = re.findall(my_regex, s[0])\n",
    "                        if len(x1)==0:         \n",
    "                            # print(\"root\")\n",
    "                            tokens[i] = tokens[i].replace(tokens[i], root[2])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def replaceArabicWords(doc):\n",
    "    doc = doc.replace('ك', 'ک')\n",
    "    doc = doc.replace('ئ', 'ی')\n",
    "    doc = doc.replace('ي', 'ی')\n",
    "    doc = doc.replace('ؤ', 'و')\n",
    "    doc = doc.replace('هٔ', 'ه')\n",
    "    doc = doc.replace('ة', 'ه')\n",
    "    doc = doc.replace('آ', 'ا')\n",
    "    doc = doc.replace('أ', 'ا')\n",
    "    doc = doc.replace('إ', 'ا')\n",
    "    return doc\n",
    "\n",
    "def pluralToSingular(tokens):\n",
    "    for i in range(0, len(tokens)):\n",
    "        for ps in plural_singular:\n",
    "            if search(ps[0], tokens[i]):\n",
    "                tokens[i] = tokens[i].replace(tokens[i], ps[1])\n",
    "    return tokens\n",
    "\n",
    "#######################################tokenization\n",
    "def tokenize(df):\n",
    "    content = df.content\n",
    "\n",
    "    tokens = []\n",
    "    for i in range(0, content.size):\n",
    "        doc = content[i]\n",
    "        # 68000 tokens\n",
    "        doc = delete_punctuations(doc)\n",
    "        # 50000 tokens\n",
    "        doc = delete_stopWords(doc)\n",
    "\n",
    "        doc = replaceArabicWords(doc)\n",
    "\n",
    "        doc = delete_postfixes(doc)\n",
    "\n",
    "        doc = delete_prefixes(doc)\n",
    "        \n",
    "\n",
    "        tokenized_doc = doc.split()\n",
    "        tokenized_doc = replaceWithRoot(tokenized_doc)\n",
    "        tokenized_doc = pluralToSingular(tokenized_doc)\n",
    "\n",
    "        for token in tokenized_doc:\n",
    "            if len(token) > 0:\n",
    "                temp = []\n",
    "                temp.append(token)\n",
    "                temp.append(df.id[i])\n",
    "                tokens.append(temp)\n",
    "    tokens.sort()\n",
    "    return tokens\n",
    "\n",
    "#######################################create inverted index\n",
    "def create_inverted_indexes(tokens):\n",
    "    inverted_indexes = []\n",
    "    doc_temp = []\n",
    "    token_temp = \"\"\n",
    "    for token in tokens:\n",
    "        if token[0] == token_temp:\n",
    "            doc_temp.append(token[1])\n",
    "        else:\n",
    "            temp = []\n",
    "            temp.append(token_temp)\n",
    "\n",
    "            (unique, counts) = np.unique(doc_temp, return_counts=True)\n",
    "            # frequencies = np.array((unique, counts), dtype='i4,f4').T.view(np.recarray)       \n",
    "            frequencies = []      \n",
    "            for i in range(len(unique)):\n",
    "                temp1 = []\n",
    "                temp1.append(unique[i])\n",
    "                temp1.append(counts[i])\n",
    "                frequencies.append(temp1)\n",
    "            temp.append(frequencies)\n",
    "            inverted_indexes.append(temp)\n",
    "            doc_temp = []\n",
    "\n",
    "        token_temp = token[0]\n",
    "        # if len(doc_temp) > 0 and token[1] != doc_temp[-1]:\n",
    "        doc_temp.append(token[1])\n",
    "    \n",
    "    inverted_indexes = delete_highFrequencyWords(inverted_indexes)\n",
    "    return inverted_indexes\n",
    "\n",
    "#######################################calculate weights\n",
    "def calculate_tfidf(df, inverted_indexes):\n",
    "    doc_length = np.zeros(len(df)+1)\n",
    "    len_df = len(df)\n",
    "    for i in range(len(inverted_indexes)):\n",
    "        if len(inverted_indexes[i][1]) > 0:\n",
    "            # print(weighted_inverted_index[i])\n",
    "            # print(len(inverted_indexes[i][1]))\n",
    "            idf = math.log(len_df/len(inverted_indexes[i][1]), 10)\n",
    "            inverted_indexes[i].append(idf)\n",
    "            # print(\"idf = \", idf)\n",
    "            for j in range(len(inverted_indexes[i][1])):\n",
    "                tf = 1 + math.log(inverted_indexes[i][1][j][1], 10)\n",
    "                # print(\"tf = \", tf)\n",
    "                w = tf*idf\n",
    "                # print(\"w : \",w)\n",
    "                inverted_indexes[i][1][j][1] = w\n",
    "                doc_length[inverted_indexes[i][1][j][0]] = doc_length[inverted_indexes[i][1][j][0]] + np.power(w, 2)\n",
    "                \n",
    "    doc_length =  np.sqrt(doc_length)\n",
    "    return inverted_indexes, doc_length\n",
    "\n",
    "#######################################create champion list\n",
    "def create_championList(inverted_indexes, r):\n",
    "    limit = r\n",
    "    championList = []\n",
    "    for i in range(len(inverted_indexes)):\n",
    "        temp = []\n",
    "        temp.append(inverted_indexes[i][0])\n",
    "        temp_postingList = list(inverted_indexes[i][1])\n",
    "        temp_postingList.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        temp1 = []\n",
    "        r = limit\n",
    "        if r > len(temp_postingList):\n",
    "            r = len(temp_postingList)\n",
    "        for j in range(r):\n",
    "            # print(temp_postingList[j])\n",
    "            temp1.append(temp_postingList[j])\n",
    "        temp.append(temp1)\n",
    "        if len(inverted_indexes[i]) == 3:\n",
    "            temp.append(inverted_indexes[i][2])\n",
    "        championList.append(temp)\n",
    "    return championList\n",
    "\n",
    "#######################################Searching\n",
    "def search_token(inverted_indexes, token_name):\n",
    "    for token in inverted_indexes:\n",
    "        if token[0] == token_name:\n",
    "            print(\"token*****: \", token[0])\n",
    "            return token[1], token[2]\n",
    "    \n",
    "    print(\"this token not exist in our database\")\n",
    "    return -1, -1\n",
    "\n",
    "def print_res(df, res):\n",
    "    p_res = pd.DataFrame()\n",
    "\n",
    "    p_res = pd.merge(df, res, on=['id'], how='inner')\n",
    "    our_res = p_res[['id', 'rank', 'url']]\n",
    "    our_res = our_res.sort_values([\"rank\"], ascending=False)\n",
    "\n",
    "    print(our_res)\n",
    "    \n",
    "\n",
    "def binary_search(df, inverted_indexes, query):\n",
    "    res = []\n",
    "    docs_id = pd.DataFrame()\n",
    "\n",
    "\n",
    "    query = delete_punctuations(query)\n",
    "    query = delete_stopWords(query)\n",
    "    query = delete_postfixes(query)\n",
    "    query = delete_prefixes(query)\n",
    "    query = replaceArabicWords(query)\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    tokenized_query = replaceWithRoot(tokenized_query)\n",
    "    tokenized_query = pluralToSingular(tokenized_query)\n",
    "\n",
    "    if len(tokenized_query) == 0:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return\n",
    "\n",
    "\n",
    "    for i in range (0 ,len(tokenized_query)):\n",
    "        docs, idf = search_token(inverted_indexes, tokenized_query[i])\n",
    "        if docs != -1:\n",
    "            temp_docs_id = pd.DataFrame(docs)\n",
    "            docs_id = pd.DataFrame(docs_id.append(temp_docs_id))\n",
    "\n",
    "    if len(docs_id) < 1:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return -1\n",
    "\n",
    "    docs_id = docs_id[0].value_counts().reset_index()\n",
    "    docs_id.columns = ['id', 'rank']\n",
    "    print_res(df, docs_id)\n",
    "\n",
    "\n",
    "def ranked_search(df, inverted_indexes, query, doc_length, k, using_heap):\n",
    "    res = []\n",
    "    docs_id = pd.DataFrame()\n",
    "    \n",
    "    query = delete_punctuations(query)\n",
    "    query = delete_stopWords(query)\n",
    "    query = delete_postfixes(query)\n",
    "    query = delete_prefixes(query)\n",
    "    query = replaceArabicWords(query)\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    tokenized_query = replaceWithRoot(tokenized_query)\n",
    "    tokenized_query = pluralToSingular(tokenized_query)\n",
    "\n",
    "    if len(tokenized_query) == 0:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return -1\n",
    "\n",
    "    (unique, counts) = np.unique(tokenized_query, return_counts=True)\n",
    "\n",
    "    tokenized_query = np.asarray((unique, counts)).T\n",
    "    # print(tokenized_query)\n",
    "\n",
    "    doc_score_temp = []\n",
    "    for i in range (0 ,len(tokenized_query)):\n",
    "        tf_query = 1 + math.log(int(tokenized_query[i][1]), 10)\n",
    "        # print(tokenized_query[i][0])\n",
    "        docs, idf = search_token(inverted_indexes, tokenized_query[i][0])\n",
    "        # print(docs)\n",
    "\n",
    "\n",
    "        if docs != -1:\n",
    "            # print(docs)\n",
    "            # print(idf)\n",
    "            w_termInQuery = tf_query*idf\n",
    "            # print(\"weight term in q: \", tf_query, idf, w_termInQuery)\n",
    "\n",
    "            for i in range(len(docs)):\n",
    "                temp = []\n",
    "                temp.append(docs[i][0])\n",
    "                # print(\"docs[i][1]: \",docs[i][1])\n",
    "                temp.append(w_termInQuery*docs[i][1])\n",
    "                doc_score_temp.append(temp)\n",
    "\n",
    "    if len(doc_score_temp) < 1:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return -1\n",
    "    doc_score_temp.sort()\n",
    "    doc_score = []\n",
    "    doc_score.append(doc_score_temp[0])\n",
    "    for i in range(1, len(doc_score_temp)):\n",
    "        if doc_score[len(doc_score)-1][0] == doc_score_temp[i][0]:\n",
    "            doc_score[len(doc_score)-1][1] = doc_score[len(doc_score)-1][1] + doc_score_temp[i][1]\n",
    "        else:\n",
    "            doc_score.append(doc_score_temp[i])\n",
    "    \n",
    "    for i in range(len(doc_score)):\n",
    "        doc_score[i][1] = doc_score[i][1] / doc_length[doc_score[i][0]]\n",
    "\n",
    "    res = []\n",
    "    if using_heap == False:\n",
    "        doc_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        if len(doc_score) > k:\n",
    "            for i in range (k):\n",
    "                res.append(doc_score[i])\n",
    "        else:\n",
    "            res = doc_score\n",
    "    if using_heap == True:\n",
    "        hp = []\n",
    "        for e in doc_score:\n",
    "            hp.append((e[1], e[0]))\n",
    "\n",
    "        nlargest = heapq.nlargest(k, hp)\n",
    "        k_high_ranked = []\n",
    "        for i in nlargest:\n",
    "            k_high_ranked.append([i[1], i[0]])\n",
    "\n",
    "        res = k_high_ranked\n",
    "\n",
    "    docs_id = pd.DataFrame(res, columns=['id', 'rank'])\n",
    "    print_res(df, docs_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_dataset(\"datasets/\", \"IR_Spring2021_ph12_7k.xlsx\")\n",
    "read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_indexes = create_inverted_indexes(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_indexes, doc_length = calculate_tfidf(df, inverted_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "championList = create_championList(inverted_indexes, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "token*****:  تیم\ntoken*****:  سپاهان\ntoken*****:  فرصت\n     id      rank                                                url\n5  1269  0.336423  https://www.isna.ir/news/98051508035/برنامه-کا...\n1   250  0.300315  https://www.isna.ir/news/99041007307/سپاهان-نی...\n2   307  0.196843  https://www.isna.ir/news/99042820646/سپاهان-مق...\n3   373  0.186529  https://www.isna.ir/news/99052216370/درد-فوتبا...\n0    58  0.120334  https://www.isna.ir/news/99012514072/ورزشکاران...\n7  3539  0.097260  https://www.isna.ir/news/99041611905/فرصتی-برا...\n9  5599  0.095828  https://www.isna.ir/news/99022719009/دردهای-شک...\n8  4391  0.089950  https://www.isna.ir/news/98020703682/ماشین-ساز...\n4   663  0.036668  https://www.isna.ir/news/99081610283/ماندگاری-...\n6  1545  0.029156  https://www.isna.ir/news/98101108272/انتقاد-به...\ntoken*****:  تیم\ntoken*****:  سپاهان\ntoken*****:  فرصت\n     id      rank                                                url\n0   181  0.476196  https://www.isna.ir/news/99032012883/سومین-تست...\n7  1480  0.460676  https://www.isna.ir/news/98090805350/حضور-فتحی...\n4   811  0.456720  https://www.isna.ir/news/99101208771/برتری-تیم...\n5  1155  0.431780  https://www.isna.ir/news/98031104941/ترخیص-مصد...\n9  1652  0.428947  https://www.isna.ir/news/98120302304/منتظر-ابل...\n3   730  0.418448  https://www.isna.ir/news/99091209903/تساوی-سای...\n6  1199  0.400267  https://www.isna.ir/news/98041206457/امیری-و-ک...\n2   644  0.399262  https://www.isna.ir/news/99081106999/طالب-ریکا...\n8  1640  0.394365  https://www.isna.ir/news/98112719763/زمان-نشست...\n1   400  0.392724  https://www.isna.ir/news/99060302867/آذری-در-ب...\ntoken*****:  تیم\ntoken*****:  سپاهان\ntoken*****:  فرصت\n        id  rank                                                url\n220    373     3  https://www.isna.ir/news/99052216370/درد-فوتبا...\n730   1199     3  https://www.isna.ir/news/98041206457/امیری-و-ک...\n228    384     3  https://www.isna.ir/news/99052720058/کریمیان-ر...\n302    493     3  https://www.isna.ir/news/99062922006/برای-جهان...\n864   1413     2  https://www.isna.ir/news/98080703460/کشتی-گیر-...\n...    ...   ...                                                ...\n517    848     1  https://www.isna.ir/news/99102519305/نتایج-رقا...\n516    847     1  https://www.isna.ir/news/99102519330/تجلیل-از-...\n515    846     1  https://www.isna.ir/news/99102519334/نتایج-روز...\n514    844     1  https://www.isna.ir/news/99102418507/درگیری-مخ...\n1458  6990     1  https://www.isna.ir/news/98091611102/هرخانه-یک...\n\n[1459 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "query = \"تیم سپاهان فرصت\"\n",
    "# query_processing(df, inverted_indexes, query)\n",
    "\n",
    "doc_score = ranked_search(df, championList, query, doc_length, k = 10, using_heap = True)\n",
    "# doc_score = ranked_search(df, championList, query, doc_length, k = 10, using_heap = False)\n",
    "\n",
    "doc_score = ranked_search(df, inverted_indexes, query, doc_length, k = 10, using_heap = True)\n",
    "\n",
    "binary_search(df, inverted_indexes, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[6768, 3.3679767852945943], [6774, 4.381838822368215], [6780, 4.381838822368215]]\n[[6774, 4.381838822368215], [6780, 4.381838822368215], [6768, 3.3679767852945943]]\n[[6768, 3.3679767852945943], [6774, 4.381838822368215], [6780, 4.381838822368215]]\n"
     ]
    }
   ],
   "source": [
    "print(inverted_indexes[1][1])\n",
    "temp_postingList = list(inverted_indexes[1][1])\n",
    "temp_postingList.sort(key=lambda x: x[1], reverse=True)\n",
    "print(temp_postingList)\n",
    "print(inverted_indexes[1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1700417\n"
     ]
    }
   ],
   "source": [
    "# print(championList[5000])\n",
    "# print(inverted_indexes[5000])\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['گفت\\u200cو\\u200cگو']\n"
     ]
    }
   ],
   "source": [
    "s = \"گفت‌و‌گو\"\n",
    "x = s.split()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<re.Match object; span=(0, 4), match='شنید'>\n"
     ]
    }
   ],
   "source": [
    "print(search('شنید', 'شنید'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "this token not exist in our database\ntoken*****:  استقلال\ntoken*****:  بازی\ntoken*****:  شنبه\ntoken*****:  پرسپولیس\ntoken*****:  چند\n      id      rank                                                url\n17  1480  0.634205  https://www.isna.ir/news/98090805350/حضور-فتحی...\n19  1613  0.621676  https://www.isna.ir/news/98111511080/فروش-۱۷-ه...\n2    213  0.601773  https://www.isna.ir/news/99033018956/موافقت-او...\n12  1042  0.595021  https://www.isna.ir/news/98010902726/شوک-بزرگ-...\n9    701  0.577228  https://www.isna.ir/news/99082820069/بازیکن-مد...\n14  1051  0.568810  https://www.isna.ir/news/98011705553/آمادگی-صی...\n1     75  0.566203  https://www.isna.ir/news/99020805715/با-دستور-...\n13  1043  0.554887  https://www.isna.ir/news/98010902616/گزارشگر-د...\n11   915  0.545152  https://www.isna.ir/news/99111309632/شکایت-است...\n18  1566  0.532947  https://www.isna.ir/news/98101713761/افشین-پیر...\n10   708  0.525139  https://www.isna.ir/news/99083020917/مهدی-خانی...\n4    287  0.514573  https://www.isna.ir/news/99042317322/احتمال-حض...\n7    640  0.506991  https://www.isna.ir/news/99081006421/پیروزی-فو...\n5    586  0.505499  https://www.isna.ir/news/99072518743/گلر-سایپا...\n6    632  0.504769  https://www.isna.ir/news/99080704492/رویارویی-...\n15  1309  0.499961  https://www.isna.ir/news/98060301227/دلیل-غیبت...\n8    700  0.491586  https://www.isna.ir/news/99082718977/عباس-بوعذ...\n16  1359  0.486740  https://www.isna.ir/news/00061215423/گزارشگر-د...\n0     26  0.483871  https://www.isna.ir/news/99010904082/آخرین-وضع...\n3    248  0.481906  https://www.isna.ir/news/99041007532/کارمندان-...\nWall time: 56.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"بازی پرسپولیس و استقلال چند شنبه است؟\"\n",
    "\n",
    "doc_score = ranked_search(df, inverted_indexes, query, doc_length, k = 20, using_heap = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "this token not exist in our database\ntoken*****:  استقلال\ntoken*****:  بازی\ntoken*****:  شنبه\ntoken*****:  پرسپولیس\ntoken*****:  چند\n      id      rank                                                url\n9   1269  0.451469  https://www.isna.ir/news/98051508035/برنامه-کا...\n6    582  0.252221  https://www.isna.ir/news/99072418213/در-آخرین-...\n5    579  0.230285  https://www.isna.ir/news/99072418443/بشار-رسن-...\n15  4068  0.204922  https://www.isna.ir/news/99100403028/بررسی-کلی...\n7    738  0.194617  https://www.isna.ir/news/99091310473/بازگشایی-...\n4    309  0.165733  https://www.isna.ir/news/99042921462/فنونی-زاد...\n2    278  0.161771  https://www.isna.ir/news/99041914462/پشت-پرده-...\n1     81  0.111597  https://www.isna.ir/news/99021309405/معاون-ساز...\n10  1404  0.098346  https://www.isna.ir/news/98080201236/به-مجموعه...\n12  1660  0.095537  https://www.isna.ir/news/98120706128/تکسیرا-در...\n8    752  0.085558  https://www.isna.ir/news/99091713534/صعود-استق...\n3    307  0.068299  https://www.isna.ir/news/99042820646/سپاهان-مق...\n11  1477  0.064877  https://www.isna.ir/news/98090704799/هواداران-...\n14  3732  0.061220  https://www.isna.ir/news/99060605182/افزایش-۱۰...\n16  4741  0.058443  https://www.isna.ir/news/98050301572/دستگیری-7...\n13  2497  0.039050  https://www.isna.ir/news/99122821736/طرح-سوال-...\n0     33  0.024497  https://www.isna.ir/news/99010200412/آغاز-دورا...\n17  5586  0.017179  https://www.isna.ir/news/99022316648/انستیتوپا...\nWall time: 35.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"بازی پرسپولیس و استقلال چند شنبه است؟\"\n",
    "\n",
    "doc_score = ranked_search(df, championList, query, doc_length, k = 20, using_heap = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['رفتن']\n"
     ]
    }
   ],
   "source": [
    "s = ['می‌روم']\n",
    "\n",
    "print(replaceWithRoot(s))"
   ]
  }
 ]
}