{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd07ec3019a7cb8f5739f8b27d55fc304c51580b268fd2b34191f81e31e11772118",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import search\n",
    "import codecs\n",
    "import sys"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    \"از\",\n",
    "    \"این\",\n",
    "    \"آن\",\n",
    "    \"به\",\n",
    "    \"با\",\n",
    "    \"بر\",\n",
    "    \"برای\",\n",
    "    \"پس\",\n",
    "    \"تا\",\n",
    "    \"در\",\n",
    "    \"را\",\n",
    "    \"که\",\n",
    "    \"و\",\n",
    "]\n",
    "\n",
    "prefixes = [\n",
    "    \"با\",\n",
    "    \"بی\",\n",
    "    \"نا\"\n",
    "]\n",
    "\n",
    "postfixes = [\n",
    "    \"تر\",\n",
    "    \"ترین\",\n",
    "    \"ات\",\n",
    "    \"ها\"\n",
    "]\n",
    "\n",
    "verb_roots = [\n",
    "    \"گفت\",\n",
    "    \"رفت\",\n",
    "    \"شد\"\n",
    "]\n",
    "\n",
    "common_words = [\n",
    "    \"میدان\",\n",
    "    \"دانش\",\n",
    "    \"رفتار\"\n",
    "]\n",
    "\n",
    "plural_singular = [\n",
    "    [\"منابع\", \"منبع\"],\n",
    "    [\"مراجع\", \"مرجع\"],\n",
    "    [\"اخبار\", \"خبر\"]\n",
    "]\n",
    "\n",
    "\n",
    "def read_dataset(path, name):\n",
    "    df = pd.read_excel(path + name)\n",
    "    return df\n",
    "\n",
    "def delete_punctuations(doc):\n",
    "    punctuations = '،:؛؟!»«()[]\"*,{.}@!?'\n",
    "    edited_doc = doc.translate(str.maketrans('', '', punctuations))\n",
    "    return edited_doc\n",
    "\n",
    "def delete_stopWords(doc):\n",
    "    edited_doc = doc\n",
    "    for s in stop_words:\n",
    "        my_regex = r\"\\b\"+s+r\"\\b\"\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def delete_highFrequencyWords(inverted_indexes):\n",
    "    df_temp = df.copy()\n",
    "    for i in inverted_indexes:\n",
    "        if len(i[1])/len(df_temp) > 0.5:\n",
    "            inverted_indexes.remove(i)\n",
    "    return inverted_indexes\n",
    "\n",
    "def delete_postfixes(doc):\n",
    "    edited_doc = doc\n",
    "    for p in postfixes:\n",
    "        my_regex = p + r\"\\b\"\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def delete_prefixes(doc):\n",
    "    edited_doc = doc\n",
    "    for p in postfixes:\n",
    "        my_regex = r\"\\b\" + p\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def replaceWithRoot(tokens):   \n",
    "    for i in range(0, len(tokens)):\n",
    "        for root in verb_roots:\n",
    "            if search(root, tokens[i]):\n",
    "                for c in common_words:\n",
    "                    if c != tokens[i]:\n",
    "                        tokens[i] = tokens[i].replace(tokens[i], root)\n",
    "                    else:\n",
    "                        print(\"Common\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def replaceArabicWords(doc):\n",
    "    doc = doc.replace('ك', 'ک')\n",
    "    doc = doc.replace('ئ', 'ی')\n",
    "    doc = doc.replace('ي', 'ی')\n",
    "    doc = doc.replace('ؤ', 'و')\n",
    "    doc = doc.replace('هٔ', 'ه')\n",
    "    doc = doc.replace('ة', 'ه')\n",
    "    doc = doc.replace('آ', 'ا')\n",
    "    doc = doc.replace('أ', 'ا')\n",
    "    doc = doc.replace('إ', 'ا')\n",
    "    return doc\n",
    "\n",
    "def pluralToSingular(tokens):\n",
    "    for i in range(0, len(tokens)):\n",
    "        for ps in plural_singular:\n",
    "            if search(ps[0], tokens[i]):\n",
    "                tokens[i] = tokens[i].replace(tokens[i], ps[1])\n",
    "    return tokens\n",
    "\n",
    "def tokenize(df):\n",
    "    content = df.content\n",
    "\n",
    "    tokens = []\n",
    "    for i in range(0, content.size):\n",
    "        doc = content[i]\n",
    "        # 68000 tokens\n",
    "        doc = delete_punctuations(doc)\n",
    "        # 50000 tokens\n",
    "        doc = delete_stopWords(doc)\n",
    "\n",
    "        doc = delete_postfixes(doc)\n",
    "\n",
    "        doc = delete_prefixes(doc)\n",
    "\n",
    "        doc = replaceArabicWords(doc)\n",
    "        \n",
    "        tokenized_doc = doc.split()\n",
    "        tokenized_doc = replaceWithRoot(tokenized_doc)\n",
    "        tokenized_doc = pluralToSingular(tokenized_doc)\n",
    "\n",
    "        for token in tokenized_doc:\n",
    "            temp = []\n",
    "            temp.append(token)\n",
    "            temp.append(df.id[i])\n",
    "            tokens.append(temp)\n",
    "    tokens.sort()\n",
    "    return tokens\n",
    "\n",
    "def create_inverted_indexes(tokens):\n",
    "    inverted_indexes = []\n",
    "    doc_temp = []\n",
    "    token_temp = \"\"\n",
    "    for token in tokens:\n",
    "        if token[0] == token_temp:\n",
    "            doc_temp.append(token[1])\n",
    "        else:\n",
    "            temp = []\n",
    "            temp.append(token_temp)\n",
    "            doc_temp = set(doc_temp)\n",
    "            temp.append(doc_temp)\n",
    "            inverted_indexes.append(temp)\n",
    "            doc_temp = []\n",
    "\n",
    "        token_temp = token[0]\n",
    "        # if len(doc_temp) > 0 and token[1] != doc_temp[-1]:\n",
    "        doc_temp.append(token[1])\n",
    "    \n",
    "    inverted_indexes = delete_highFrequencyWords(inverted_indexes)\n",
    "    return inverted_indexes\n",
    "\n",
    "def search_token(inverted_indexes, token_name):\n",
    "    for token in inverted_indexes:\n",
    "        if token[0] == token_name:\n",
    "            return token[1]\n",
    "    print(\"this token not exist in our database\")\n",
    "\n",
    "def query_processing(df, inverted_indexes, query):\n",
    "    res = []\n",
    "    docs_id = pd.DataFrame()\n",
    "\n",
    "\n",
    "    query = delete_punctuations(query)\n",
    "    query = delete_stopWords(query)\n",
    "    query = delete_postfixes(query)\n",
    "    query = delete_prefixes(query)\n",
    "    query = replaceArabicWords(query)\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    tokenized_query = replaceWithRoot(tokenized_query)\n",
    "    tokenized_query = pluralToSingular(tokenized_query)\n",
    "\n",
    "\n",
    "    for i in range (0 ,len(tokenized_query)):\n",
    "        temp_docs_id = pd.DataFrame(search_token(inverted_indexes, tokenized_query[i]))\n",
    "        docs_id = pd.DataFrame(docs_id.append(temp_docs_id))\n",
    "\n",
    "    docs_id = docs_id[0].value_counts().reset_index()\n",
    "    docs_id.columns = ['id', 'rank']\n",
    "    print_res(df, docs_id)\n",
    "    \n",
    "def print_res(df, res):\n",
    "    p_res = pd.DataFrame()\n",
    "\n",
    "    p_res = pd.merge(df, res, on=['id'], how='inner')\n",
    "    our_res = p_res[['id', 'rank', 'url']]\n",
    "    our_res = our_res.sort_values([\"rank\"], ascending=False)\n",
    "\n",
    "    print(our_res)\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_dataset(\"datasets/\", \"IR_Spring2021_ph12_7k.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(df)\n",
    "inverted_indexes = create_inverted_indexes(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1711466\n47433\n"
     ]
    }
   ],
   "source": [
    "print(len(search_token(inverted_indexes, \"خبر\")))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "872\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(inverted_indexes))\n",
    "\n",
    "# for i in range(5000, 5100):\n",
    "#     print(inverted_indexes[i][0])\n",
    "#     print(inverted_indexes[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        id  rank                                                url\n118    220     5  https://www.isna.ir/news/99040302540/۴۸-سالگی-...\n831   1363     3  https://www.isna.ir/news/98070302858/اتلتیکو-ب...\n1076  1697     3  https://www.isna.ir/news/98121713587/توقف-اتلت...\n738   1202     3  https://www.isna.ir/news/98041306951/خداحافظی-...\n987   1580     2  https://www.isna.ir/news/98102620428/میشو-تصمی...\n...    ...   ...                                                ...\n480    796     1  https://www.isna.ir/news/99100604356/صحبت-های-...\n481    797     1  https://www.isna.ir/news/99100604215/انتقاد-حس...\n482    799     1  https://www.isna.ir/news/99100704952/پیکر-دکتر...\n484    801     1  https://www.isna.ir/news/99100805979/استقلال-و...\n1249  6990     1  https://www.isna.ir/news/98091611102/هرخانه-یک...\n\n[1250 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "query = \"بازیکن فرانسوی به تیم کهکشانی مادرید\"\n",
    "query_processing(df, inverted_indexes, query)\n"
   ]
  }
 ]
}