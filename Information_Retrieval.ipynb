{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd07ec3019a7cb8f5739f8b27d55fc304c51580b268fd2b34191f81e31e11772118",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import search\n",
    "import codecs\n",
    "import sys\n",
    "import string\n",
    "import math\n",
    "import heapq\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "prefixes = []\n",
    "postfixes = []\n",
    "verb_roots = []\n",
    "common_words = []\n",
    "plural_singular = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################importing\n",
    "def read_dataset(path, name):\n",
    "    df = pd.read_excel(path + name)\n",
    "    return df\n",
    "\n",
    "def read_files():\n",
    "    path = 'files/'\n",
    "\n",
    "    #stop_words\n",
    "    name = 'stop_words.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        stop_words.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #prefixes\n",
    "    name = 'prefixes.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        prefixes.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #postfixes\n",
    "    name = 'postfixes.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        postfixes.append(line)\n",
    "    f.close()\n",
    "\n",
    "    # verb_roots\n",
    "    name = 'verb.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    count = 0\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        line = line.split('-')\n",
    "        verb_roots.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #common_words\n",
    "    name = 'common_words.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        common_words.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #plural_singular\n",
    "    name = 'plural_singular.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    count = 0\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        line = line.split('-')\n",
    "        plural_singular.append(line)\n",
    "    f.close()\n",
    "\n",
    "#######################################Normalization\n",
    "def delete_punctuations(doc):\n",
    "    punctuations = '؟،:؛!»«()[]*,{.}@!?،؛<>#$&!~\"\\|-_+'\n",
    "    edited_doc = doc.translate(str.maketrans('', '', punctuations))\n",
    "    edited_doc = edited_doc.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return edited_doc\n",
    "\n",
    "def delete_stopWords(doc):\n",
    "    edited_doc = doc\n",
    "    for s in stop_words:\n",
    "        my_regex = r\"\\b\"+s+r\"\\b\"\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def delete_highFrequencyWords(inverted_indexes):\n",
    "    for i in inverted_indexes:\n",
    "        if len(i[1])/len(df) > 0.8:\n",
    "            inverted_indexes.remove(i)\n",
    "    return inverted_indexes\n",
    "\n",
    "def delete_postfixes(doc):\n",
    "    edited_doc = doc\n",
    "    for p in postfixes:\n",
    "        my_regex = p + r\"\\b\"\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def delete_prefixes(doc):\n",
    "    edited_doc = doc\n",
    "    for p in postfixes:\n",
    "        my_regex = r\"\\b\" + p\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def replaceWithRoot(tokens): \n",
    "    for i in range(0, len(tokens)):\n",
    "        # print(verb_roots)\n",
    "        for root in verb_roots:\n",
    "            if search(root[0], tokens[i]) or search(root[1], tokens[i]):\n",
    "                common = False\n",
    "                for c in common_words:\n",
    "                    if c == tokens[i]:\n",
    "                        common = True\n",
    "                        # print(\"common\")\n",
    "                        break\n",
    "                if common == False:\n",
    "                    my_regex = r\"\\b^\"+root[1]+r\"\\.*\"\n",
    "                    x = re.findall(my_regex, tokens[i])\n",
    " \n",
    "                    if len(x) == 0:  \n",
    "                        my_regex = r\"\\b...*\"+root[1]+r\"\\b\"\n",
    "                        x1 = re.findall(my_regex, tokens[i])\n",
    "                        if len(x1)==0:         \n",
    "                            # print(\"root\")\n",
    "                            tokens[i] = tokens[i].replace(tokens[i], root[2])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def replaceArabicWords(doc):\n",
    "    doc = doc.replace('ك', 'ک')\n",
    "    doc = doc.replace('ئ', 'ی')\n",
    "    doc = doc.replace('ي', 'ی')\n",
    "    doc = doc.replace('ؤ', 'و')\n",
    "    doc = doc.replace('هٔ', 'ه')\n",
    "    doc = doc.replace('ة', 'ه')\n",
    "    doc = doc.replace('آ', 'ا')\n",
    "    doc = doc.replace('أ', 'ا')\n",
    "    doc = doc.replace('إ', 'ا')\n",
    "    return doc\n",
    "\n",
    "def pluralToSingular(tokens):\n",
    "    for i in range(0, len(tokens)):\n",
    "        for ps in plural_singular:\n",
    "            if search(ps[0], tokens[i]):\n",
    "                tokens[i] = tokens[i].replace(tokens[i], ps[1])\n",
    "    return tokens\n",
    "\n",
    "#######################################tokenization\n",
    "def tokenize(df):\n",
    "    content = df.content\n",
    "\n",
    "    tokens = []\n",
    "    for i in range(0, content.size):\n",
    "        doc = content[i]\n",
    "        # 68000 tokens\n",
    "        doc = delete_punctuations(doc)\n",
    "        # 50000 tokens\n",
    "        doc = delete_stopWords(doc)\n",
    "\n",
    "        doc = replaceArabicWords(doc)\n",
    "\n",
    "        doc = delete_postfixes(doc)\n",
    "\n",
    "        doc = delete_prefixes(doc)\n",
    "        \n",
    "\n",
    "        tokenized_doc = doc.split()\n",
    "        tokenized_doc = replaceWithRoot(tokenized_doc)\n",
    "        tokenized_doc = pluralToSingular(tokenized_doc)\n",
    "\n",
    "        for token in tokenized_doc:\n",
    "            if len(token) > 0:\n",
    "                temp = []\n",
    "                temp.append(token)\n",
    "                temp.append(df.id[i])\n",
    "                tokens.append(temp)\n",
    "    # tokens.sort()\n",
    "    return tokens\n",
    "\n",
    "#######################################create inverted index\n",
    "def create_inverted_indexes(tokens):\n",
    "    tokens.sort()\n",
    "    inverted_indexes = []\n",
    "    doc_temp = []\n",
    "    token_temp = \"\"\n",
    "    for token in tokens:\n",
    "        if token[0] == token_temp:\n",
    "            doc_temp.append(token[1])\n",
    "        else:\n",
    "            if len(token[0])>0:\n",
    "                temp = []\n",
    "                temp.append(token_temp)\n",
    "\n",
    "                (unique, counts) = np.unique(doc_temp, return_counts=True)\n",
    "                # frequencies = np.array((unique, counts), dtype='i4,f4').T.view(np.recarray)       \n",
    "                frequencies = []      \n",
    "                for i in range(len(unique)):\n",
    "                    temp1 = []\n",
    "                    temp1.append(unique[i])\n",
    "                    temp1.append(counts[i])\n",
    "                    frequencies.append(temp1)\n",
    "                temp.append(frequencies)\n",
    "                inverted_indexes.append(temp)\n",
    "                doc_temp = []\n",
    "                doc_temp.append(token[1])\n",
    "\n",
    "        token_temp = token[0]\n",
    "        # if len(doc_temp) > 0 and token[1] != doc_temp[-1]:\n",
    "    \n",
    "    inverted_indexes = delete_highFrequencyWords(inverted_indexes)\n",
    "    return inverted_indexes\n",
    "\n",
    "#######################################calculate weights\n",
    "def calculate_tfidf(df, inverted_indexes):\n",
    "    doc_length = np.zeros(len(df)+1)\n",
    "    len_df = len(df)\n",
    "    for i in range(len(inverted_indexes)):\n",
    "        if len(inverted_indexes[i][1]) > 0:\n",
    "            # print(weighted_inverted_index[i])\n",
    "            # print(len(inverted_indexes[i][1]))\n",
    "            idf = math.log(len_df/len(inverted_indexes[i][1]), 10)\n",
    "            inverted_indexes[i].append(idf)\n",
    "            # print(\"idf = \", idf)\n",
    "            for j in range(len(inverted_indexes[i][1])):\n",
    "                tf = 1 + math.log(inverted_indexes[i][1][j][1], 10)\n",
    "                # print(\"tf = \", tf)\n",
    "                w = tf*idf\n",
    "                # print(\"w : \",w)\n",
    "                inverted_indexes[i][1][j][1] = w\n",
    "                # if inverted_indexes[i][1][j][0] == 6545:\n",
    "                #     print(\"6545: \", inverted_indexes[i][1][j])\n",
    "                #     print(\"6545: \", inverted_indexes[i][0])\n",
    "                # if inverted_indexes[i][1][j][0] == 6550:\n",
    "                #     print(\"6550: \", inverted_indexes[i][1][j])\n",
    "                #     print(\"6550: \", inverted_indexes[i][0])\n",
    "\n",
    "\n",
    "                doc_length[inverted_indexes[i][1][j][0]] = doc_length[inverted_indexes[i][1][j][0]] + np.power(w, 2)\n",
    "                \n",
    "    doc_length =  np.sqrt(doc_length)\n",
    "    return inverted_indexes, doc_length\n",
    "\n",
    "#######################################create champion list\n",
    "def create_championList(inverted_indexes, r):\n",
    "    limit = r\n",
    "    championList = []\n",
    "    for i in range(len(inverted_indexes)):\n",
    "        temp = []\n",
    "        temp.append(inverted_indexes[i][0])\n",
    "        temp_postingList = list(inverted_indexes[i][1])\n",
    "        temp_postingList.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        temp1 = []\n",
    "        r = limit\n",
    "        if r > len(temp_postingList):\n",
    "            r = len(temp_postingList)\n",
    "        for j in range(r):\n",
    "            # print(temp_postingList[j])\n",
    "            temp1.append(temp_postingList[j])\n",
    "        temp.append(temp1)\n",
    "        if len(inverted_indexes[i]) == 3:\n",
    "            temp.append(inverted_indexes[i][2])\n",
    "        championList.append(temp)\n",
    "    return championList\n",
    "\n",
    "#######################################Searching\n",
    "def search_token(inverted_indexes, token_name):\n",
    "    # for token in inverted_indexes:\n",
    "    #     if token[0] == token_name:\n",
    "    #         # print(\"token*****: \", token[0])\n",
    "    #         return token[1], token[2]\n",
    "    if token_name in inverted_indexes:\n",
    "        return inverted_indexes[token_name][0], inverted_indexes[token_name][1]\n",
    "    # print(\"this token not exist in our database\")\n",
    "    return -1, -1\n",
    "\n",
    "def print_res(df, res):\n",
    "    p_res = pd.DataFrame()\n",
    "\n",
    "    p_res = pd.merge(df, res, on=['id'], how='inner')\n",
    "    if 'topic' in df:\n",
    "        our_res = p_res[['id', 'rank', 'topic', 'url']]\n",
    "    else:\n",
    "        our_res = p_res[['id', 'rank', 'url']]\n",
    "    our_res = our_res.sort_values([\"rank\"], ascending=False)\n",
    "\n",
    "    return(our_res)\n",
    "    \n",
    "\n",
    "def binary_search(df, inverted_indexes, query):\n",
    "    res = []\n",
    "    docs_id = pd.DataFrame()\n",
    "\n",
    "\n",
    "    query = delete_punctuations(query)\n",
    "    query = delete_stopWords(query)\n",
    "    query = delete_postfixes(query)\n",
    "    query = delete_prefixes(query)\n",
    "    query = replaceArabicWords(query)\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    tokenized_query = replaceWithRoot(tokenized_query)\n",
    "    tokenized_query = pluralToSingular(tokenized_query)\n",
    "\n",
    "    if len(tokenized_query) == 0:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return\n",
    "\n",
    "\n",
    "    for i in range (0 ,len(tokenized_query)):\n",
    "        docs, idf = search_token(inverted_indexes, tokenized_query[i])\n",
    "        if docs != -1:\n",
    "            temp_docs_id = pd.DataFrame(docs)\n",
    "            docs_id = pd.DataFrame(docs_id.append(temp_docs_id))\n",
    "\n",
    "    if len(docs_id) < 1:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return -1\n",
    "\n",
    "    docs_id = docs_id[0].value_counts().reset_index()\n",
    "    docs_id.columns = ['id', 'rank']\n",
    "    df_res = print_res(df, docs_id)\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def ranked_search(df, inverted_indexes, query, doc_length, k, using_heap):\n",
    "    res = []\n",
    "    docs_id = pd.DataFrame()\n",
    "    try:\n",
    "        using_class = False\n",
    "        pattern = \"cat\\:(.*?)\\ \"\n",
    "        topic = re.search(pattern, query).group(1)\n",
    "        if len(topic) > 0:\n",
    "            print(topic)\n",
    "            using_class = True\n",
    "    except AttributeError:\n",
    "        topic = re.search(pattern, query)\n",
    "    \n",
    "    query = delete_punctuations(query)\n",
    "    query = delete_stopWords(query)\n",
    "    query = delete_postfixes(query)\n",
    "    query = delete_prefixes(query)\n",
    "    query = replaceArabicWords(query)\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    tokenized_query = replaceWithRoot(tokenized_query)\n",
    "    tokenized_query = pluralToSingular(tokenized_query)\n",
    "\n",
    "    if len(tokenized_query) == 0:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return -1\n",
    "\n",
    "    (unique, counts) = np.unique(tokenized_query, return_counts=True)\n",
    "\n",
    "    tokenized_query = np.asarray((unique, counts)).T\n",
    "    # print(tokenized_query)\n",
    "\n",
    "    doc_score_temp = []\n",
    "    for i in range (0 ,len(tokenized_query)):\n",
    "        tf_query = 1 + math.log(int(tokenized_query[i][1]), 10)\n",
    "        # print(tokenized_query[i][0])\n",
    "        docs, idf = search_token(inverted_indexes, tokenized_query[i][0])\n",
    "        # print(docs)\n",
    "\n",
    "\n",
    "        if docs != -1:\n",
    "            # print(docs)\n",
    "            # print(idf)\n",
    "            w_termInQuery = tf_query*idf\n",
    "            # print(\"weight term in q: \", tf_query, idf, w_termInQuery)\n",
    "\n",
    "            for i in range(len(docs)):\n",
    "                temp = []\n",
    "                if using_class == True:\n",
    "                    if dict_topics[docs[i][0]] == topic:\n",
    "                        temp.append(docs[i][0])\n",
    "                        # print(\"docs[i][1]: \",docs[i][1])\n",
    "                        temp.append(w_termInQuery*docs[i][1])\n",
    "                        doc_score_temp.append([docs[i][0], w_termInQuery*docs[i][1]])\n",
    "                else:\n",
    "                    temp.append(docs[i][0])\n",
    "                    # print(\"docs[i][1]: \",docs[i][1])\n",
    "                    temp.append(w_termInQuery*docs[i][1])\n",
    "                    doc_score_temp.append([docs[i][0], w_termInQuery*docs[i][1]])\n",
    "\n",
    "    if len(doc_score_temp) < 1:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return -1\n",
    "    doc_score_temp.sort()\n",
    "    doc_score = []\n",
    "    doc_score.append(doc_score_temp[0])\n",
    "    for i in range(1, len(doc_score_temp)):\n",
    "        if doc_score[len(doc_score)-1][0] == doc_score_temp[i][0]:\n",
    "            doc_score[len(doc_score)-1][1] = doc_score[len(doc_score)-1][1] + doc_score_temp[i][1]\n",
    "        else:\n",
    "            doc_score.append(doc_score_temp[i])\n",
    "    \n",
    "    # for i in range(len(doc_score)):\n",
    "    #     doc_score[i][1] = doc_score[i][1] / doc_length[doc_score[i][0]]\n",
    "\n",
    "    res = []\n",
    "    if using_heap == False:\n",
    "        doc_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        if len(doc_score) > k:\n",
    "            for i in range (k):\n",
    "                res.append(doc_score[i])\n",
    "        else:\n",
    "            res = doc_score\n",
    "    if using_heap == True:\n",
    "        hp = []\n",
    "        for e in doc_score:\n",
    "            hp.append((e[1], e[0]))\n",
    "\n",
    "        nlargest = heapq.nlargest(k, hp)\n",
    "        k_high_ranked = []\n",
    "        for i in nlargest:\n",
    "            k_high_ranked.append([i[1], i[0]])\n",
    "\n",
    "        res = k_high_ranked\n",
    "\n",
    "    docs_id = pd.DataFrame(res, columns=['id', 'rank'])\n",
    "    res_df = print_res(df, docs_id)\n",
    "    return res_df\n",
    "\n",
    "#######################################phase3 Clustering\n",
    "def k_means(k, iteration):\n",
    "    #select k centroids randomly\n",
    "    centroids = []\n",
    "    randoms = np.random.randint(1, len(inverted_index_perDoc) , size=(k))\n",
    "    for i in range(len(randoms)):\n",
    "        centroids.append(inverted_index_perDoc[randoms[i]-1][1])\n",
    "    # print(centroids)\n",
    "\n",
    "    max_index_row = 0\n",
    "    cluster_docs = []       \n",
    "    for it in range(iteration):\n",
    "        print(it)\n",
    "        doc_membership = np.zeros((len(df), k))\n",
    "        for c in range(len(centroids)):\n",
    "            print(\"******c= \",c)\n",
    "            for term in centroids[c]:\n",
    "            # for key,value in centroids[c].items():\n",
    "                # print(dict_inverted_indexes)\n",
    "                docs, idf = search_token(dict_inverted_indexes, term[0])\n",
    "                if docs != -1:\n",
    "                    for doc in docs:\n",
    "                        doc_membership[doc[0]-1][c] = doc_membership[doc[0]-1][c] + term[1]*doc[1]\n",
    "        #update centroids\n",
    "\n",
    "        temp_index_max = max_index_row\n",
    "        max_index_row = np.argmax(doc_membership, axis=1)\n",
    "        if np.array_equal(temp_index_max, max_index_row):\n",
    "            print(\"centroids not changed\")\n",
    "            cluster_name = np.argmax(doc_membership, axis=1)\n",
    "            for c in range(len(centroids)):\n",
    "                cluster_docs.append(np.where(cluster_name == c))\n",
    "\n",
    "            return centroids, doc_membership, cluster_docs\n",
    "\n",
    "        centroids = []\n",
    "        for i in range(k):\n",
    "            # print(\"******k= \",i)\n",
    "            doc_id_perCluster = np.where(max_index_row == i)\n",
    "            # print(doc_id_perCluster)\n",
    "            temp = []\n",
    "            for doc_id in doc_id_perCluster[0]:\n",
    "                # print(doc_id)\n",
    "                # print(\"hey\")\n",
    "                for term in inverted_index_perDoc[doc_id][1]:\n",
    "                    temp.append(term)\n",
    "            centroids.append(temp)\n",
    "        for i in range(k):\n",
    "            # data_items = centroids[i].items()\n",
    "            # data_list = list(data_items)\n",
    "            df_centroid = pd.DataFrame(centroids[i], columns=['term','weight'])\n",
    "            df_centroid = df_centroid.groupby('term', as_index=False)['weight'].mean()\n",
    "            centroids[i] = df_centroid.to_numpy()\n",
    "            # centroids[i] = dict(centroids[i]) \n",
    "\n",
    "        print(doc_membership)\n",
    "        cluster_name = np.argmax(doc_membership, axis=1)\n",
    "        for c in range(len(centroids)):\n",
    "            cluster_docs.append(np.where(cluster_name == c))\n",
    "\n",
    "    return centroids, doc_membership, cluster_docs\n",
    "\n",
    "def ranked_search_withClustering(df, inverted_indexes, query, doc_length, k, using_heap, b2):\n",
    "    res = []\n",
    "    docs_id = pd.DataFrame()\n",
    "    \n",
    "    query = delete_punctuations(query)\n",
    "    query = delete_stopWords(query)\n",
    "    query = delete_postfixes(query)\n",
    "    query = delete_prefixes(query)\n",
    "    query = replaceArabicWords(query)\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    tokenized_query = replaceWithRoot(tokenized_query)\n",
    "    tokenized_query = pluralToSingular(tokenized_query)\n",
    "\n",
    "    if len(tokenized_query) == 0:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return -1\n",
    "\n",
    "    (unique, counts) = np.unique(tokenized_query, return_counts=True)\n",
    "\n",
    "    tokenized_query = np.asarray((unique, counts)).T\n",
    "    # print(tokenized_query)\n",
    "    doc_score_temp = []\n",
    "    docs_id = np.array\n",
    "    for i in range (0 ,len(tokenized_query)):\n",
    "        tf_query = 1 + math.log(int(tokenized_query[i][1]), 10)\n",
    "        # print(tokenized_query[i][0])\n",
    "        docs, idf = search_token(inverted_indexes, tokenized_query[i][0])\n",
    "        similarityToCentroids = np.zeros(len(centroids))\n",
    "        w_termInQuery = tf_query*idf\n",
    "        for c in range(len(centroids)):\n",
    "            if tokenized_query[i][0] in centroids[c]:\n",
    "                similarityToCentroids[c] = similarityToCentroids[c] + w_termInQuery*centroids[c][tokenized_query[i][0]]\n",
    "    for b in range(b2):\n",
    "        # print(similarityToCentroids)\n",
    "        mostSimilar = np.argmax(similarityToCentroids)\n",
    "        # print(mostSimilar)\n",
    "        similarityToCentroids[mostSimilar] = 0\n",
    "\n",
    "        if b == 0:\n",
    "            docs_id = cluster_docs[mostSimilar][0]\n",
    "        else:\n",
    "            docs_id_temp = cluster_docs[mostSimilar][0]\n",
    "            docs_id = np.append(docs_id, docs_id_temp)\n",
    "\n",
    "    # print(\"*****\")\n",
    "    # print(docs_id)\n",
    "    # print(len(docs_id))\n",
    "\n",
    "    # print(similarityToCentroids)\n",
    "    #search in docs\n",
    "    doc_score_temp = []\n",
    "    doc_score = []\n",
    "    for i in range (0 ,len(tokenized_query)):\n",
    "        # print(tokenized_query[i][0])\n",
    "        tf_query = 1 + math.log(int(tokenized_query[i][1]), 10)\n",
    "        # print(tokenized_query[i][0])\n",
    "        docs, idf = search_token(inverted_indexes, tokenized_query[i][0])\n",
    "\n",
    "        w_termInQuery = tf_query*idf\n",
    "        # print(len(docs_id))\n",
    "        for doc_id in docs_id:\n",
    "            if tokenized_query[i][0] in inverted_index_perDoc[doc_id][1]:\n",
    "                # print(doc_id)\n",
    "                # print(w_termInQuery)\n",
    "                # print(inverted_index_perDoc[doc_id][1][tokenized_query[i][0]])\n",
    "                doc_score_temp.append([doc_id + 1, inverted_index_perDoc[doc_id][1][tokenized_query[i][0]] * w_termInQuery])\n",
    "    if len(doc_score_temp) < 1:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return -1\n",
    "\n",
    "    doc_score_temp.sort()\n",
    "    # print(doc_score_temp)\n",
    "\n",
    "    doc_score.append(doc_score_temp[0])\n",
    "    # print(len(doc_score_temp))\n",
    "    for i in range(1, len(doc_score_temp)):\n",
    "        if doc_score[len(doc_score)-1][0] == doc_score_temp[i][0]:\n",
    "            # print(doc_score_temp[i][0])\n",
    "            # print(doc_score[len(doc_score)-1][1])\n",
    "            doc_score[len(doc_score)-1][1] = doc_score[len(doc_score)-1][1] + doc_score_temp[i][1]\n",
    "            # print(doc_score[len(doc_score)-1][1])\n",
    "        else:\n",
    "            doc_score.append(doc_score_temp[i])\n",
    "\n",
    "\n",
    "    # for i in range(len(doc_score)):\n",
    "    #     doc_score[i][1] = doc_score[i][1] / doc_length[doc_score[i][0]]\n",
    "\n",
    "    res = []\n",
    "    if using_heap == False:\n",
    "        doc_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        if len(doc_score) > k:\n",
    "            for i in range (k):\n",
    "                res.append(doc_score[i])\n",
    "        else:\n",
    "            res = doc_score\n",
    "    if using_heap == True:\n",
    "        hp = []\n",
    "        for e in doc_score:\n",
    "            hp.append((e[1], e[0]))\n",
    "\n",
    "        nlargest = heapq.nlargest(k, hp)\n",
    "        k_high_ranked = []\n",
    "        for i in nlargest:\n",
    "            k_high_ranked.append([i[1], i[0]])\n",
    "\n",
    "        res = k_high_ranked\n",
    "\n",
    "\n",
    "    docs_id = pd.DataFrame(res, columns=['id', 'rank'])\n",
    "    df_res = print_res(df, docs_id)\n",
    "    return df_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_dataset(\"datasets/\", \"IR_Spring2021_ph12_7k.xlsx\")\n",
    "read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_indexes = create_inverted_indexes(tokens)\n",
    "inverted_indexes.pop(0)\n",
    "inverted_indexes, doc_length = calculate_tfidf(df, inverted_indexes)\n",
    "#normalize inverted_index\n",
    "for i in range(len(inverted_indexes)):\n",
    "    for j in range(len(inverted_indexes[i][1])):\n",
    "        inverted_indexes[i][1][j][1] = inverted_indexes[i][1][j][1] / doc_length[inverted_indexes[i][1][j][0]]\n",
    "\n",
    "championList = create_championList(inverted_indexes, 4)\n",
    "# print(inverted_indexes[1])\n",
    "\n",
    "##create dictionaries\n",
    "dict_inverted_indexes = []\n",
    "for i in range(len(inverted_indexes)):\n",
    "    temp = []\n",
    "    temp.append(inverted_indexes[i][0])\n",
    "    temp.append([inverted_indexes[i][1], inverted_indexes[i][2]])\n",
    "    dict_inverted_indexes.append(temp)\n",
    "# inverted_indexes = dict(inverted_indexes)\n",
    "# print(inverted_indexes['سلام'])\n",
    "dict_championList = []\n",
    "for i in range(len(championList)):\n",
    "    temp = []\n",
    "    temp.append(championList[i][0])\n",
    "    temp.append([championList[i][1], championList[i][2]])\n",
    "    dict_championList.append(temp)\n",
    "\n",
    "dict_inverted_indexes = dict(dict_inverted_indexes)\n",
    "dict_championList = dict(dict_championList)\n",
    "\n",
    "#create inverted_index_perDoc\n",
    "doc_term = []\n",
    "for i in range(len(inverted_indexes)):\n",
    "    for j in range(len(inverted_indexes[i][1])):\n",
    "        doc_term.append([inverted_indexes[i][1][j][0], inverted_indexes[i][0], inverted_indexes[i][1][j][1]])\n",
    "\n",
    "doc_term.sort()\n",
    "# print(doc_term)\n",
    "inverted_index_perDoc = []\n",
    "temp = 0\n",
    "temp = doc_term[0][0]\n",
    "temp1 = []\n",
    "for i in range(0, len(doc_term)):\n",
    "    # print(inverted_index_perDoc[len(inverted_index_perDoc)][0])\n",
    "    if temp == doc_term[i][0]:\n",
    "        temp1.append([doc_term[i][1], doc_term[i][2]])\n",
    "    else:\n",
    "        inverted_index_perDoc.append([temp, temp1])\n",
    "        temp = []\n",
    "        temp1 = []\n",
    "\n",
    "        temp = doc_term[i][0]\n",
    "        temp1.append([doc_term[i][1], doc_term[i][2]])\n",
    "inverted_index_perDoc.append([temp, temp1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "[[0.04001389 0.02329522 0.02738133]\n",
      " [0.02794589 0.05533207 0.03104033]\n",
      " [0.02920601 0.02437369 0.03906508]\n",
      " ...\n",
      " [0.02495452 0.02042171 0.02620888]\n",
      " [0.01563316 0.01225014 0.00050611]\n",
      " [0.02608012 0.00713049 0.01064599]]\n",
      "1\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "[[0.98477332 0.73656944 0.85232959]\n",
      " [0.32333577 0.74633045 0.44392708]\n",
      " [0.74707482 0.87021454 0.95952203]\n",
      " ...\n",
      " [0.95587182 0.88792889 1.3453247 ]\n",
      " [0.83112024 0.52151624 0.55442936]\n",
      " [0.72714284 0.52834399 0.6427835 ]]\n",
      "2\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "[[0.97400641 0.75598187 0.86397991]\n",
      " [0.31455189 0.75507084 0.44769246]\n",
      " [0.72794316 0.88564431 0.98389311]\n",
      " ...\n",
      " [0.9292604  0.92514989 1.35897526]\n",
      " [0.82471641 0.54407439 0.55545747]\n",
      " [0.72000213 0.54699161 0.64963583]]\n",
      "3\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "[[0.96418401 0.76441528 0.84990146]\n",
      " [0.30786664 0.74526147 0.44332636]\n",
      " [0.7083426  0.87800147 0.96883992]\n",
      " ...\n",
      " [0.92173998 0.94451526 1.35670063]\n",
      " [0.82007409 0.54027531 0.55991827]\n",
      " [0.71922672 0.61608468 0.67235365]]\n",
      "4\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "[[0.97207315 0.76255722 0.8289375 ]\n",
      " [0.30809995 0.7305831  0.43597567]\n",
      " [0.69265607 0.86577351 0.96164462]\n",
      " ...\n",
      " [0.92575272 0.93148616 1.35044529]\n",
      " [0.82434779 0.52904748 0.55070677]\n",
      " [0.72333502 0.57667414 0.66658305]]\n",
      "5\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "[[0.97077361 0.76221973 0.82825373]\n",
      " [0.30758228 0.72745705 0.43514793]\n",
      " [0.6894901  0.86876707 0.95992441]\n",
      " ...\n",
      " [0.92776729 0.93064339 1.34846987]\n",
      " [0.82444882 0.52904933 0.54961384]\n",
      " [0.72872386 0.57606015 0.6606816 ]]\n",
      "6\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "[[0.97140952 0.76033968 0.82802677]\n",
      " [0.30785371 0.72973627 0.43007462]\n",
      " [0.68872186 0.86863787 0.95860619]\n",
      " ...\n",
      " [0.92797063 0.9297879  1.34796186]\n",
      " [0.82458796 0.52875241 0.5496026 ]\n",
      " [0.72886692 0.57523994 0.6606141 ]]\n",
      "7\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "centroids not changed\n"
     ]
    }
   ],
   "source": [
    "centroids, doc_membership, cluster_docs = k_means(3, 12)\n",
    "for i in range(len(inverted_index_perDoc)):\n",
    "    inverted_index_perDoc[i][1] = dict(inverted_index_perDoc[i][1])\n",
    "\n",
    "for i in range(len(centroids)):\n",
    "    centroids[i] = dict(centroids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     id      rank                                                url\n8  6997  0.393855  https://www.isna.ir/news/98092015327/انواع-دیا...\n9  7000  0.393855  https://www.isna.ir/news/98092015327/انواع-دیا...\n6  6873  0.334320  https://www.isna.ir/news/98071410719/دلایل-ابت...\n7  6879  0.334320  https://www.isna.ir/news/98071410719/دلایل-ابت...\n4  6671  0.271924  https://www.isna.ir/news/98032210328/تولید-کرم...\n5  6673  0.271924  https://www.isna.ir/news/98032210328/تولید-کرم...\n0  5981  0.267747  https://www.isna.ir/news/99061914723/نکاتی-که-...\n1  5989  0.267747  https://www.isna.ir/news/99061914723/نکاتی-که-...\n2  6555  0.259982  https://www.isna.ir/news/98010300572/زنان-بارد...\n3  6559  0.259982  https://www.isna.ir/news/98010300572/زنان-بارد...\nhealth\n     id      rank                                                url\n8  6997  0.393855  https://www.isna.ir/news/98092015327/انواع-دیا...\n9  7000  0.393855  https://www.isna.ir/news/98092015327/انواع-دیا...\n6  6873  0.334320  https://www.isna.ir/news/98071410719/دلایل-ابت...\n7  6879  0.334320  https://www.isna.ir/news/98071410719/دلایل-ابت...\n4  6671  0.271924  https://www.isna.ir/news/98032210328/تولید-کرم...\n5  6673  0.271924  https://www.isna.ir/news/98032210328/تولید-کرم...\n0  5981  0.267747  https://www.isna.ir/news/99061914723/نکاتی-که-...\n1  5989  0.267747  https://www.isna.ir/news/99061914723/نکاتی-که-...\n2  6555  0.259982  https://www.isna.ir/news/98010300572/زنان-بارد...\n3  6559  0.259982  https://www.isna.ir/news/98010300572/زنان-بارد...\n"
     ]
    }
   ],
   "source": [
    "query = \"حاملگی\"\n",
    "# query_processing(df, inverted_indexes, query)\n",
    "\n",
    "# print(ranked_search(df, dict_championList, query, doc_length, k = 10, using_heap = True))\n",
    "# doc_score = ranked_search(df, championList, query, doc_length, k = 10, using_heap = False)\n",
    "# print(ranked_search_withClustering(df, dict_inverted_indexes, query, doc_length, k = 10, using_heap = True, b2 = 3))\n",
    "\n",
    "print(ranked_search(df, dict_inverted_indexes, query, doc_length, k = 10, using_heap = True))\n",
    "query = \"cat:health حاملگی\"\n",
    "\n",
    "print(ranked_search(df, dict_inverted_indexes, query, doc_length, k = 10, using_heap = True))\n",
    "\n",
    "\n",
    "# print(binary_search(df, dict_inverted_indexes, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ['میرو']\n",
    "\n",
    "print(replaceWithRoot(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# high_idf_tokens = []\n",
    "# for i in range(len(inverted_indexes)):\n",
    "#     if len(inverted_indexes[i]) == 3:\n",
    "#         if inverted_indexes[i][2] > 3.84:\n",
    "#             high_idf_tokens.append(inverted_indexes[i])\n",
    "#             print(inverted_indexes[i])\n",
    "# print(len(high_idf_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_invertedIndex = []\n",
    "# for i in range(len(inverted_indexes)):\n",
    "#     df_invertedIndex.append([inverted_indexes[i][0], pd.DataFrame(inverted_indexes[i][1], columns=['term','weight'])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_11k = read_dataset(\"datasets/\", \"IR00_3_11k News.xlsx\")\n",
    "df_17k = read_dataset(\"datasets/\", \"IR00_3_17k News.xlsx\")\n",
    "df_20k = read_dataset(\"datasets/\", \"IR00_3_20k News.xlsx\")\n",
    "df_50k = pd.concat([df_11k, df_17k, df_20k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50k['id']  = range(1, 1 + len(df_50k))\n",
    "# duplicates_loaded = len(df_50k[df_50k.duplicated(subset=['url', 'content', 'topic'])])\n",
    "# print(duplicates_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50k = df_50k.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_50k = tokenize(df_50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2900745\n7631414\n6365851\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens_11k))\n",
    "print(len(tokens_17k))\n",
    "print(len(tokens_20k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16898010"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "len(tokens_50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_indexes_50k = create_inverted_indexes(tokens_50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inverted_indexes_50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "243026"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "inverted_indexes_50k[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['', []]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "inverted_indexes_50k.pop(0)\n",
    "inverted_indexes_50k, doc_length_50k = calculate_tfidf(df_50k, inverted_indexes_50k)\n",
    "#normalize inverted_index\n",
    "for i in range(len(inverted_indexes_50k)):\n",
    "    for j in range(len(inverted_indexes_50k[i][1])):\n",
    "        inverted_indexes_50k[i][1][j][1] = inverted_indexes_50k[i][1][j][1] / doc_length_50k[inverted_indexes_50k[i][1][j][0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_inverted_indexes_50k = []\n",
    "for i in range(len(inverted_indexes_50k)):\n",
    "    temp = []\n",
    "    temp.append(inverted_indexes_50k[i][0])\n",
    "    temp.append([inverted_indexes_50k[i][1], inverted_indexes_50k[i][2]])\n",
    "    dict_inverted_indexes_50k.append(temp)\n",
    "# inverted_indexes = dict(inverted_indexes)\n",
    "# print(inverted_indexes['سلام'])\n",
    "# dict_championList = []\n",
    "# for i in range(len(championList)):\n",
    "#     temp = []\n",
    "#     temp.append(championList[i][0])\n",
    "#     temp.append([championList[i][1], championList[i][2]])\n",
    "#     dict_championList.append(temp)\n",
    "\n",
    "dict_inverted_indexes_50k = dict(dict_inverted_indexes_50k)\n",
    "# dict_championList = dict(dict_championList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "243025"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "df_50k['topic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['sport', 'politics', 'economy', 'health', 'culture'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "# df_50k.loc[df_50k['topic'] == 'sports'] = df_50k.loc[df_50k['topic'] == 'sports'].topic.replace('sport')\n",
    "df_50k['topic'] = df_50k['topic'].replace('sports','sport')\n",
    "df_50k['topic'] = df_50k['topic'].replace('political','politics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          id                                            content     topic  \\\n",
       "0          1  به گزارش ایسنا، پس از استعفا علی نکویی رئیس شا...     sport   \n",
       "1          2  به گزارش ایسنا، امیر محسنی با بیان اینکه این ل...     sport   \n",
       "2          3  آرش فرهادیان در گفت و گو با ایسنا، درباره آخری...     sport   \n",
       "3          4  به گزارش ایسنا، فدراسیون بین المللی شنا قانونی...     sport   \n",
       "4          5  به گزارش ایسنا، فدراسیون جهانی صعودهای ورزشی (...     sport   \n",
       "...      ...                                                ...       ...   \n",
       "50056  50057  به گزارش حوزه پارلمانی خبرگزاری فارس، [محمدباق...  politics   \n",
       "50057  50058  به گزارش خبرنگار تشکل‌های دانشگاهی خبرگزاری فا...  politics   \n",
       "50058  50059  به گزارش خبرنگار حوزه دولت خبرگزاری فارس، رضا ...  politics   \n",
       "50059  50060  به گزارش خبرنگار دولت خبرگزاری فارس، محسن حاجی...  politics   \n",
       "50060  50061  حجت‌الاسلام علیرضا سلیمی نماینده مردم محلات در...  politics   \n",
       "\n",
       "                                                     url  \n",
       "0      https://www.isna.ir/news/99010100077/حواشی-در-...  \n",
       "1      https://www.isna.ir/news/98122922468/ثبت-نام-ب...  \n",
       "2      https://www.isna.ir/news/99010200541/فرهادیان-...  \n",
       "3      https://www.isna.ir/news/99010200528/فناوری-وی...  \n",
       "4      https://www.isna.ir/news/99010200510/تعویق-زما...  \n",
       "...                                                  ...  \n",
       "50056  https://www.farsnews.ir/news/13990612000494/پی...  \n",
       "50057  https://www.farsnews.ir/news/13990612000397/اع...  \n",
       "50058  https://www.farsnews.ir/news/13990612000425/اف...  \n",
       "50059  https://www.farsnews.ir/news/13990612000435/وز...  \n",
       "50060  https://www.farsnews.ir/news/13990611001190/سل...  \n",
       "\n",
       "[50061 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>content</th>\n      <th>topic</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>به گزارش ایسنا، پس از استعفا علی نکویی رئیس شا...</td>\n      <td>sport</td>\n      <td>https://www.isna.ir/news/99010100077/حواشی-در-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>به گزارش ایسنا، امیر محسنی با بیان اینکه این ل...</td>\n      <td>sport</td>\n      <td>https://www.isna.ir/news/98122922468/ثبت-نام-ب...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>آرش فرهادیان در گفت و گو با ایسنا، درباره آخری...</td>\n      <td>sport</td>\n      <td>https://www.isna.ir/news/99010200541/فرهادیان-...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>به گزارش ایسنا، فدراسیون بین المللی شنا قانونی...</td>\n      <td>sport</td>\n      <td>https://www.isna.ir/news/99010200528/فناوری-وی...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>به گزارش ایسنا، فدراسیون جهانی صعودهای ورزشی (...</td>\n      <td>sport</td>\n      <td>https://www.isna.ir/news/99010200510/تعویق-زما...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>50056</th>\n      <td>50057</td>\n      <td>به گزارش حوزه پارلمانی خبرگزاری فارس، [محمدباق...</td>\n      <td>politics</td>\n      <td>https://www.farsnews.ir/news/13990612000494/پی...</td>\n    </tr>\n    <tr>\n      <th>50057</th>\n      <td>50058</td>\n      <td>به گزارش خبرنگار تشکل‌های دانشگاهی خبرگزاری فا...</td>\n      <td>politics</td>\n      <td>https://www.farsnews.ir/news/13990612000397/اع...</td>\n    </tr>\n    <tr>\n      <th>50058</th>\n      <td>50059</td>\n      <td>به گزارش خبرنگار حوزه دولت خبرگزاری فارس، رضا ...</td>\n      <td>politics</td>\n      <td>https://www.farsnews.ir/news/13990612000425/اف...</td>\n    </tr>\n    <tr>\n      <th>50059</th>\n      <td>50060</td>\n      <td>به گزارش خبرنگار دولت خبرگزاری فارس، محسن حاجی...</td>\n      <td>politics</td>\n      <td>https://www.farsnews.ir/news/13990612000435/وز...</td>\n    </tr>\n    <tr>\n      <th>50060</th>\n      <td>50061</td>\n      <td>حجت‌الاسلام علیرضا سلیمی نماینده مردم محلات در...</td>\n      <td>politics</td>\n      <td>https://www.farsnews.ir/news/13990611001190/سل...</td>\n    </tr>\n  </tbody>\n</table>\n<p>50061 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "len(dict_inverted_indexes_50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dict_topics = {}\n",
    "for i in range(len(df):\n",
    "    query = df.content[i]\n",
    "    # print(query)\n",
    "    res = ranked_search(df_50k, dict_inverted_indexes_50k, query, doc_length_50k, k = 9, using_heap = True)\n",
    "    className = res['topic'].value_counts().idxmax()\n",
    "    dict_topics[i+1] = className\n",
    "    if i%1000 == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "Wall time: 18min 30s\n"
     ]
    }
   ],
   "source": [
    "dict_topics[2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'economy'"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "import pickle\n",
    "path = 'files/'\n",
    "a_file = open(path + \"IR_Spring2021_ph12_7k_lablesDict.pkl\", \"wb\")\n",
    "pickle.dump(dict_topics, a_file)\n",
    "a_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(path + \"IR_Spring2021_ph12_7k_lablesDict.pkl\", \"rb\")\n",
    "output = pickle.load(a_file)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "alth', 5768: 'health', 5769: 'health', 5770: 'health', 5771: 'health', 5772: 'health', 5773: 'politics', 5774: 'politics', 5775: 'health', 5776: 'health', 5777: 'health', 5778: 'health', 5779: 'health', 5780: 'health', 5781: 'health', 5782: 'health', 5783: 'health', 5784: 'health', 5785: 'health', 5786: 'health', 5787: 'health', 5788: 'health', 5789: 'sport', 5790: 'health', 5791: 'health', 5792: 'health', 5793: 'health', 5794: 'health', 5795: 'health', 5796: 'health', 5797: 'health', 5798: 'health', 5799: 'health', 5800: 'politics', 5801: 'health', 5802: 'health', 5803: 'economy', 5804: 'health', 5805: 'politics', 5806: 'health', 5807: 'politics', 5808: 'politics', 5809: 'health', 5810: 'health', 5811: 'health', 5812: 'health', 5813: 'health', 5814: 'health', 5815: 'health', 5816: 'politics', 5817: 'politics', 5818: 'health', 5819: 'health', 5820: 'health', 5821: 'health', 5822: 'health', 5823: 'economy', 5824: 'health', 5825: 'health', 5826: 'health', 5827: 'economy', 5828: 'health', 5829: 'health', 5830: 'economy', 5831: 'health', 5832: 'health', 5833: 'health', 5834: 'health', 5835: 'health', 5836: 'health', 5837: 'politics', 5838: 'culture', 5839: 'politics', 5840: 'health', 5841: 'health', 5842: 'health', 5843: 'health', 5844: 'health', 5845: 'health', 5846: 'health', 5847: 'health', 5848: 'health', 5849: 'health', 5850: 'health', 5851: 'health', 5852: 'health', 5853: 'health', 5854: 'health', 5855: 'politics', 5856: 'politics', 5857: 'health', 5858: 'culture', 5859: 'health', 5860: 'health', 5861: 'health', 5862: 'health', 5863: 'culture', 5864: 'health', 5865: 'health', 5866: 'health', 5867: 'health', 5868: 'politics', 5869: 'health', 5870: 'culture', 5871: 'politics', 5872: 'health', 5873: 'health', 5874: 'culture', 5875: 'health', 5876: 'health', 5877: 'health', 5878: 'politics', 5879: 'health', 5880: 'health', 5881: 'health', 5882: 'health', 5883: 'health', 5884: 'health', 5885: 'health', 5886: 'health', 5887: 'health', 5888: 'health', 5889: 'health', 5890: 'health', 5891: 'health', 5892: 'politics', 5893: 'health', 5894: 'health', 5895: 'health', 5896: 'culture', 5897: 'politics', 5898: 'health', 5899: 'health', 5900: 'health', 5901: 'health', 5902: 'health', 5903: 'health', 5904: 'health', 5905: 'sport', 5906: 'health', 5907: 'politics', 5908: 'health', 5909: 'health', 5910: 'health', 5911: 'health', 5912: 'health', 5913: 'politics', 5914: 'health', 5915: 'health', 5916: 'health', 5917: 'health', 5918: 'health', 5919: 'culture', 5920: 'health', 5921: 'health', 5922: 'health', 5923: 'health', 5924: 'culture', 5925: 'health', 5926: 'health', 5927: 'health', 5928: 'health', 5929: 'health', 5930: 'health', 5931: 'health', 5932: 'health', 5933: 'health', 5934: 'health', 5935: 'health', 5936: 'health', 5937: 'health', 5938: 'health', 5939: 'health', 5940: 'health', 5941: 'health', 5942: 'health', 5943: 'health', 5944: 'politics', 5945: 'health', 5946: 'health', 5947: 'health', 5948: 'health', 5949: 'health', 5950: 'economy', 5951: 'health', 5952: 'health', 5953: 'health', 5954: 'health', 5955: 'economy', 5956: 'health', 5957: 'health', 5958: 'health', 5959: 'health', 5960: 'health', 5961: 'health', 5962: 'health', 5963: 'health', 5964: 'health', 5965: 'health', 5966: 'health', 5967: 'culture', 5968: 'health', 5969: 'health', 5970: 'politics', 5971: 'politics', 5972: 'health', 5973: 'health', 5974: 'health', 5975: 'health', 5976: 'health', 5977: 'health', 5978: 'politics', 5979: 'health', 5980: 'health', 5981: 'health', 5982: 'health', 5983: 'health', 5984: 'politics', 5985: 'health', 5986: 'health', 5987: 'health', 5988: 'health', 5989: 'health', 5990: 'health', 5991: 'health', 5992: 'health', 5993: 'health', 5994: 'health', 5995: 'health', 5996: 'health', 5997: 'health', 5998: 'health', 5999: 'health', 6000: 'health', 6001: 'culture', 6002: 'health', 6003: 'health', 6004: 'health', 6005: 'health', 6006: 'politics', 6007: 'health', 6008: 'health', 6009: 'health', 6010: 'health', 6011: 'health', 6012: 'health', 6013: 'health', 6014: 'health', 6015: 'health', 6016: 'health', 6017: 'health', 6018: 'health', 6019: 'health', 6020: 'health', 6021: 'health', 6022: 'health', 6023: 'health', 6024: 'health', 6025: 'health', 6026: 'health', 6027: 'politics', 6028: 'health', 6029: 'politics', 6030: 'health', 6031: 'health', 6032: 'health', 6033: 'health', 6034: 'health', 6035: 'health', 6036: 'health', 6037: 'health', 6038: 'health', 6039: 'health', 6040: 'health', 6041: 'health', 6042: 'health', 6043: 'health', 6044: 'health', 6045: 'health', 6046: 'health', 6047: 'health', 6048: 'health', 6049: 'health', 6050: 'health', 6051: 'culture', 6052: 'health', 6053: 'health', 6054: 'health', 6055: 'politics', 6056: 'health', 6057: 'politics', 6058: 'health', 6059: 'health', 6060: 'health', 6061: 'health', 6062: 'culture', 6063: 'health', 6064: 'health', 6065: 'health', 6066: 'politics', 6067: 'health', 6068: 'culture', 6069: 'health', 6070: 'health', 6071: 'politics', 6072: 'health', 6073: 'health', 6074: 'health', 6075: 'health', 6076: 'health', 6077: 'health', 6078: 'health', 6079: 'health', 6080: 'health', 6081: 'health', 6082: 'health', 6083: 'health', 6084: 'health', 6085: 'health', 6086: 'health', 6087: 'economy', 6088: 'health', 6089: 'health', 6090: 'health', 6091: 'health', 6092: 'health', 6093: 'health', 6094: 'politics', 6095: 'health', 6096: 'health', 6097: 'politics', 6098: 'health', 6099: 'health', 6100: 'health', 6101: 'politics', 6102: 'culture', 6103: 'health', 6104: 'health', 6105: 'health', 6106: 'politics', 6107: 'health', 6108: 'health', 6109: 'health', 6110: 'health', 6111: 'health', 6112: 'health', 6113: 'health', 6114: 'health', 6115: 'health', 6116: 'health', 6117: 'politics', 6118: 'health', 6119: 'health', 6120: 'health', 6121: 'economy', 6122: 'health', 6123: 'health', 6124: 'politics', 6125: 'health', 6126: 'health', 6127: 'health', 6128: 'health', 6129: 'health', 6130: 'health', 6131: 'health', 6132: 'health', 6133: 'health', 6134: 'politics', 6135: 'health', 6136: 'politics', 6137: 'health', 6138: 'health', 6139: 'health', 6140: 'health', 6141: 'health', 6142: 'health', 6143: 'health', 6144: 'politics', 6145: 'politics', 6146: 'health', 6147: 'health', 6148: 'health', 6149: 'health', 6150: 'health', 6151: 'health', 6152: 'health', 6153: 'health', 6154: 'culture', 6155: 'health', 6156: 'politics', 6157: 'health', 6158: 'health', 6159: 'health', 6160: 'politics', 6161: 'health', 6162: 'health', 6163: 'health', 6164: 'health', 6165: 'health', 6166: 'culture', 6167: 'health', 6168: 'health', 6169: 'health', 6170: 'health', 6171: 'culture', 6172: 'health', 6173: 'health', 6174: 'health', 6175: 'health', 6176: 'health', 6177: 'economy', 6178: 'health', 6179: 'health', 6180: 'health', 6181: 'health', 6182: 'health', 6183: 'health', 6184: 'health', 6185: 'health', 6186: 'health', 6187: 'health', 6188: 'health', 6189: 'health', 6190: 'health', 6191: 'health', 6192: 'health', 6193: 'economy', 6194: 'health', 6195: 'health', 6196: 'health', 6197: 'health', 6198: 'politics', 6199: 'health', 6200: 'health', 6201: 'health', 6202: 'culture', 6203: 'health', 6204: 'health', 6205: 'health', 6206: 'health', 6207: 'health', 6208: 'politics', 6209: 'health', 6210: 'culture', 6211: 'health', 6212: 'health', 6213: 'health', 6214: 'health', 6215: 'health', 6216: 'health', 6217: 'health', 6218: 'health', 6219: 'health', 6220: 'health', 6221: 'health', 6222: 'health', 6223: 'health', 6224: 'health', 6225: 'health', 6226: 'politics', 6227: 'health', 6228: 'health', 6229: 'politics', 6230: 'health', 6231: 'health', 6232: 'politics', 6233: 'culture', 6234: 'health', 6235: 'health', 6236: 'health', 6237: 'health', 6238: 'health', 6239: 'health', 6240: 'health', 6241: 'health', 6242: 'health', 6243: 'health', 6244: 'health', 6245: 'health', 6246: 'health', 6247: 'health', 6248: 'politics', 6249: 'health', 6250: 'health', 6251: 'culture', 6252: 'health', 6253: 'health', 6254: 'health', 6255: 'health', 6256: 'health', 6257: 'culture', 6258: 'health', 6259: 'politics', 6260: 'health', 6261: 'health', 6262: 'health', 6263: 'health', 6264: 'health', 6265: 'culture', 6266: 'health', 6267: 'health', 6268: 'health', 6269: 'health', 6270: 'health', 6271: 'culture', 6272: 'health', 6273: 'health', 6274: 'health', 6275: 'culture', 6276: 'health', 6277: 'health', 6278: 'health', 6279: 'politics', 6280: 'health', 6281: 'health', 6282: 'health', 6283: 'health', 6284: 'health', 6285: 'economy', 6286: 'health', 6287: 'culture', 6288: 'health', 6289: 'health', 6290: 'culture', 6291: 'health', 6292: 'health', 6293: 'politics', 6294: 'culture', 6295: 'health', 6296: 'culture', 6297: 'health', 6298: 'health', 6299: 'health', 6300: 'health', 6301: 'health', 6302: 'health', 6303: 'health', 6304: 'culture', 6305: 'health', 6306: 'health', 6307: 'health', 6308: 'health', 6309: 'health', 6310: 'health', 6311: 'health', 6312: 'health', 6313: 'health', 6314: 'health', 6315: 'health', 6316: 'health', 6317: 'health', 6318: 'health', 6319: 'health', 6320: 'health', 6321: 'health', 6322: 'health', 6323: 'health', 6324: 'health', 6325: 'health', 6326: 'health', 6327: 'health', 6328: 'health', 6329: 'health', 6330: 'health', 6331: 'health', 6332: 'health', 6333: 'health', 6334: 'health', 6335: 'health', 6336: 'politics', 6337: 'health', 6338: 'health', 6339: 'health', 6340: 'politics', 6341: 'health', 6342: 'health', 6343: 'health', 6344: 'health', 6345: 'health', 6346: 'health', 6347: 'health', 6348: 'health', 6349: 'health', 6350: 'health', 6351: 'health', 6352: 'health', 6353: 'politics', 6354: 'health', 6355: 'health', 6356: 'politics', 6357: 'health', 6358: 'health', 6359: 'health', 6360: 'health', 6361: 'health', 6362: 'health', 6363: 'health', 6364: 'health', 6365: 'health', 6366: 'health', 6367: 'health', 6368: 'health', 6369: 'health', 6370: 'health', 6371: 'health', 6372: 'politics', 6373: 'health', 6374: 'politics', 6375: 'health', 6376: 'health', 6377: 'health', 6378: 'health', 6379: 'health', 6380: 'health', 6381: 'health', 6382: 'health', 6383: 'culture', 6384: 'health', 6385: 'health', 6386: 'health', 6387: 'health', 6388: 'culture', 6389: 'health', 6390: 'health', 6391: 'health', 6392: 'health', 6393: 'health', 6394: 'health', 6395: 'health', 6396: 'politics', 6397: 'health', 6398: 'health', 6399: 'health', 6400: 'health', 6401: 'health', 6402: 'health', 6403: 'politics', 6404: 'health', 6405: 'economy', 6406: 'economy', 6407: 'health', 6408: 'health', 6409: 'politics', 6410: 'health', 6411: 'politics', 6412: 'health', 6413: 'health', 6414: 'politics', 6415: 'health', 6416: 'economy', 6417: 'health', 6418: 'health', 6419: 'health', 6420: 'health', 6421: 'economy', 6422: 'health', 6423: 'health', 6424: 'health', 6425: 'health', 6426: 'culture', 6427: 'health', 6428: 'health', 6429: 'health', 6430: 'economy', 6431: 'health', 6432: 'health', 6433: 'culture', 6434: 'health', 6435: 'health', 6436: 'health', 6437: 'economy', 6438: 'health', 6439: 'health', 6440: 'culture', 6441: 'health', 6442: 'health', 6443: 'politics', 6444: 'health', 6445: 'health', 6446: 'health', 6447: 'culture', 6448: 'health', 6449: 'politics', 6450: 'health', 6451: 'health', 6452: 'health', 6453: 'culture', 6454: 'health', 6455: 'health', 6456: 'health', 6457: 'health', 6458: 'health', 6459: 'health', 6460: 'health', 6461: 'economy', 6462: 'health', 6463: 'health', 6464: 'health', 6465: 'health', 6466: 'health', 6467: 'health', 6468: 'health', 6469: 'health', 6470: 'health', 6471: 'health', 6472: 'health', 6473: 'health', 6474: 'health', 6475: 'health', 6476: 'health', 6477: 'health', 6478: 'health', 6479: 'health', 6480: 'health', 6481: 'health', 6482: 'health', 6483: 'health', 6484: 'health', 6485: 'health', 6486: 'health', 6487: 'health', 6488: 'health', 6489: 'economy', 6490: 'health', 6491: 'health', 6492: 'health', 6493: 'health', 6494: 'health', 6495: 'health', 6496: 'culture', 6497: 'health', 6498: 'health', 6499: 'health', 6500: 'health', 6501: 'health', 6502: 'culture', 6503: 'health', 6504: 'health', 6505: 'health', 6506: 'health', 6507: 'politics', 6508: 'health', 6509: 'health', 6510: 'health', 6511: 'health', 6512: 'politics', 6513: 'health', 6514: 'health', 6515: 'culture', 6516: 'health', 6517: 'politics', 6518: 'health', 6519: 'health', 6520: 'culture', 6521: 'health', 6522: 'health', 6523: 'health', 6524: 'politics', 6525: 'health', 6526: 'health', 6527: 'politics', 6528: 'health', 6529: 'health', 6530: 'health', 6531: 'health', 6532: 'health', 6533: 'health', 6534: 'politics', 6535: 'health', 6536: 'health', 6537: 'health', 6538: 'health', 6539: 'health', 6540: 'health', 6541: 'health', 6542: 'health', 6543: 'health', 6544: 'health', 6545: 'health', 6546: 'health', 6547: 'health', 6548: 'health', 6549: 'health', 6550: 'health', 6551: 'health', 6552: 'health', 6553: 'health', 6554: 'health', 6555: 'health', 6556: 'health', 6557: 'health', 6558: 'health', 6559: 'health', 6560: 'health', 6561: 'health', 6562: 'health', 6563: 'health', 6564: 'health', 6565: 'health', 6566: 'health', 6567: 'health', 6568: 'health', 6569: 'health', 6570: 'health', 6571: 'health', 6572: 'health', 6573: 'health', 6574: 'health', 6575: 'health', 6576: 'health', 6577: 'health', 6578: 'health', 6579: 'health', 6580: 'politics', 6581: 'health', 6582: 'health', 6583: 'health', 6584: 'health', 6585: 'health', 6586: 'politics', 6587: 'health', 6588: 'health', 6589: 'health', 6590: 'health', 6591: 'politics', 6592: 'health', 6593: 'health', 6594: 'health', 6595: 'health', 6596: 'health', 6597: 'economy', 6598: 'health', 6599: 'health', 6600: 'health', 6601: 'economy', 6602: 'health', 6603: 'health', 6604: 'economy', 6605: 'health', 6606: 'health', 6607: 'health', 6608: 'health', 6609: 'health', 6610: 'health', 6611: 'health', 6612: 'health', 6613: 'health', 6614: 'health', 6615: 'health', 6616: 'health', 6617: 'health', 6618: 'health', 6619: 'health', 6620: 'health', 6621: 'health', 6622: 'health', 6623: 'health', 6624: 'health', 6625: 'politics', 6626: 'politics', 6627: 'health', 6628: 'politics', 6629: 'politics', 6630: 'economy', 6631: 'health', 6632: 'economy', 6633: 'health', 6634: 'health', 6635: 'economy', 6636: 'health', 6637: 'economy', 6638: 'health', 6639: 'health', 6640: 'health', 6641: 'economy', 6642: 'health', 6643: 'health', 6644: 'health', 6645: 'health', 6646: 'health', 6647: 'health', 6648: 'health', 6649: 'health', 6650: 'health', 6651: 'health', 6652: 'health', 6653: 'health', 6654: 'health', 6655: 'health', 6656: 'health', 6657: 'health', 6658: 'health', 6659: 'health', 6660: 'health', 6661: 'health', 6662: 'health', 6663: 'health', 6664: 'health', 6665: 'health', 6666: 'health', 6667: 'health', 6668: 'health', 6669: 'health', 6670: 'health', 6671: 'health', 6672: 'health', 6673: 'health', 6674: 'health', 6675: 'health', 6676: 'politics', 6677: 'health', 6678: 'health', 6679: 'politics', 6680: 'health', 6681: 'health', 6682: 'health', 6683: 'health', 6684: 'health', 6685: 'health', 6686: 'health', 6687: 'health', 6688: 'health', 6689: 'health', 6690: 'health', 6691: 'health', 6692: 'health', 6693: 'health', 6694: 'health', 6695: 'health', 6696: 'health', 6697: 'health', 6698: 'health', 6699: 'health', 6700: 'health', 6701: 'health', 6702: 'health', 6703: 'health', 6704: 'economy', 6705: 'health', 6706: 'health', 6707: 'health', 6708: 'health', 6709: 'health', 6710: 'economy', 6711: 'health', 6712: 'health', 6713: 'health', 6714: 'health', 6715: 'culture', 6716: 'economy', 6717: 'health', 6718: 'health', 6719: 'health', 6720: 'economy', 6721: 'health', 6722: 'health', 6723: 'health', 6724: 'health', 6725: 'health', 6726: 'culture', 6727: 'health', 6728: 'health', 6729: 'culture', 6730: 'health', 6731: 'health', 6732: 'culture', 6733: 'economy', 6734: 'politics', 6735: 'health', 6736: 'politics', 6737: 'health', 6738: 'health', 6739: 'health', 6740: 'health', 6741: 'health', 6742: 'health', 6743: 'health', 6744: 'health', 6745: 'health', 6746: 'health', 6747: 'health', 6748: 'health', 6749: 'health', 6750: 'health', 6751: 'health', 6752: 'health', 6753: 'health', 6754: 'health', 6755: 'health', 6756: 'health', 6757: 'health', 6758: 'health', 6759: 'health', 6760: 'health', 6761: 'health', 6762: 'health', 6763: 'health', 6764: 'health', 6765: 'health', 6766: 'health', 6767: 'health', 6768: 'health', 6769: 'health', 6770: 'health', 6771: 'health', 6772: 'health', 6773: 'health', 6774: 'health', 6775: 'health', 6776: 'health', 6777: 'health', 6778: 'health', 6779: 'health', 6780: 'health', 6781: 'health', 6782: 'politics', 6783: 'health', 6784: 'health', 6785: 'health', 6786: 'health', 6787: 'culture', 6788: 'health', 6789: 'health', 6790: 'health', 6791: 'health', 6792: 'health', 6793: 'health', 6794: 'health', 6795: 'health', 6796: 'health', 6797: 'health', 6798: 'health', 6799: 'health', 6800: 'health', 6801: 'health', 6802: 'health', 6803: 'health', 6804: 'health', 6805: 'health', 6806: 'health', 6807: 'health', 6808: 'health', 6809: 'health', 6810: 'health', 6811: 'health', 6812: 'health', 6813: 'health', 6814: 'health', 6815: 'health', 6816: 'health', 6817: 'health', 6818: 'health', 6819: 'health', 6820: 'health', 6821: 'health', 6822: 'health', 6823: 'health', 6824: 'health', 6825: 'culture', 6826: 'health', 6827: 'health', 6828: 'culture', 6829: 'health', 6830: 'health', 6831: 'culture', 6832: 'health', 6833: 'health', 6834: 'health', 6835: 'culture', 6836: 'health', 6837: 'health', 6838: 'health', 6839: 'health', 6840: 'health', 6841: 'health', 6842: 'health', 6843: 'economy', 6844: 'health', 6845: 'health', 6846: 'health', 6847: 'economy', 6848: 'health', 6849: 'health', 6850: 'health', 6851: 'health', 6852: 'health', 6853: 'politics', 6854: 'health', 6855: 'health', 6856: 'health', 6857: 'health', 6858: 'health', 6859: 'health', 6860: 'economy', 6861: 'health', 6862: 'health', 6863: 'health', 6864: 'health', 6865: 'health', 6866: 'health', 6867: 'health', 6868: 'health', 6869: 'health', 6870: 'health', 6871: 'health', 6872: 'health', 6873: 'health', 6874: 'health', 6875: 'health', 6876: 'health', 6877: 'health', 6878: 'health', 6879: 'health', 6880: 'health', 6881: 'health', 6882: 'health', 6883: 'health', 6884: 'health', 6885: 'health', 6886: 'health', 6887: 'health', 6888: 'health', 6889: 'health', 6890: 'health', 6891: 'health', 6892: 'health', 6893: 'health', 6894: 'health', 6895: 'health', 6896: 'health', 6897: 'health', 6898: 'health', 6899: 'health', 6900: 'health', 6901: 'health', 6902: 'health', 6903: 'health', 6904: 'health', 6905: 'health', 6906: 'health', 6907: 'health', 6908: 'health', 6909: 'health', 6910: 'health', 6911: 'health', 6912: 'health', 6913: 'health', 6914: 'health', 6915: 'health', 6916: 'health', 6917: 'health', 6918: 'health', 6919: 'culture', 6920: 'health', 6921: 'health', 6922: 'health', 6923: 'health', 6924: 'health', 6925: 'health', 6926: 'health', 6927: 'health', 6928: 'health', 6929: 'health', 6930: 'health', 6931: 'health', 6932: 'health', 6933: 'health', 6934: 'health', 6935: 'health', 6936: 'health', 6937: 'health', 6938: 'health', 6939: 'health', 6940: 'health', 6941: 'health', 6942: 'politics', 6943: 'health', 6944: 'health', 6945: 'politics', 6946: 'health', 6947: 'health', 6948: 'health', 6949: 'health', 6950: 'health', 6951: 'health', 6952: 'health', 6953: 'health', 6954: 'health', 6955: 'health', 6956: 'health', 6957: 'health', 6958: 'health', 6959: 'health', 6960: 'health', 6961: 'culture', 6962: 'culture', 6963: 'health', 6964: 'health', 6965: 'health', 6966: 'health', 6967: 'health', 6968: 'health', 6969: 'health', 6970: 'health', 6971: 'health', 6972: 'health', 6973: 'health', 6974: 'health', 6975: 'health', 6976: 'health', 6977: 'health', 6978: 'health', 6979: 'health', 6980: 'health', 6981: 'health', 6982: 'health', 6983: 'health', 6984: 'health', 6985: 'economy', 6986: 'health', 6987: 'health', 6988: 'economy', 6989: 'health', 6990: 'health', 6991: 'politics', 6992: 'health', 6993: 'health', 6994: 'politics', 6995: 'health', 6996: 'health', 6997: 'health', 6998: 'culture', 6999: 'health', 7000: 'health'}\n"
     ]
    }
   ],
   "source": [
    "dict_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{1: 'sport', 2: 'sport', 3: 'sport'}"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "df_11k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'economy'"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "dfkk['id']  = range(10, 10 + len(dfkk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_11k\n",
    "duplicates_loaded = len(df_50k[df_50k.duplicated(subset=['url'])])\n",
    "print(duplicates_loaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1313\n"
     ]
    }
   ],
   "source": [
    "x['topic'].value_counts().idxmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "heyy\n"
     ]
    }
   ],
   "source": [
    "if 'topic' in df_50k:\n",
    "    print(\"heyy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "به گزارش ایسنا پس از استعفا علی نکویی رئیس شاخه آکروباتیک ژیمناستیک ایران به\nدلیل اختلافات با رئیس فدراسیون ژیمناستیک دبیر این شاخه هم استعفا داد\nآکروباتیک ژیمناستیک ایران که به تازگی موفق شده مجوز حضور بانوان را در عرصه های\nبین المللی کسب کند بعد از استعفا رئیس این شاخه دچار حواشی زیادی شده است\n\nکریم محمدزاده که از سال ۱۳۹۴ در این فدراسیون در سمت دبیر شاخه آکروباتیک\nژیمناستیک مشغول به کار بوده است بعد از استعفا نکویی از سمت خود کناره‌گیری\nکرد در دوره حضور او به عنوان دبیر در این شاخه ملی پوشان آکروباتیک ایران موفق\nشده بودند که هشت مدال آسیایی را کسب کنند\n\nدر متن استعفای کریم محمد زاده خطاب به رئیس فدراسیون آمده است\n\nباسلام بدینوسیله اینجانب کریم محمدزاده که از سال ۱۳۹۴ در این فدراسیون در سمت\nدبیر شاخه آکروباتیک ژیمناستیک مشغول به تلاش شبانه روزی می‌باشم تقاضای استعفای\nخود را از تاریخ ۲۶ اسفند ۱۳۹۸ به حضور محترم اعلام می نمایم\n\nخدا را شکر اینجانب به اتفاق مجموعه همکاران توانستیم نتایج خوبی در این مدت از\nفعالیت ها که بر همه آشکار است رقم بزنیم و از رهنمودها و آموزشهای اساتید بزرگ\nبهره مند شدم و امیدوارم همکاری بنده نیز برای این شاخه مثمر ثمر بوده باشد\n\nدر نهایت صداقت و حسن نیت خواهشمند است دستور فرمایید در ارتباط با درخواست\nاینجانب جهت تسویه حساب اقدام لازم را مبذول نماید\n\nضمن عذر خواهی از خانواده فهیم و بزرگ آکروباتیک ژیمناستیک کشور از حسن نظر\nجنابعالی کمال تشکر را داشته و توفیق روز افزون شما و همکاران محترم شاخه\nآکروباتیک را از خداوند متعال خواستارم\n\nبا تشکر و دعای خیر کریم محمدزاده\n\nهمچنین صابر یار محمدلویی که حکم مسئولیت روابط عمومی شاخه آکروباتیک را از نکویی\nدریافت کرده بود به دلیل اینکه از زحمات و تلاشهای رییس مستعفی شاخه آکروباتیک\nقدردانی کرده بود مجبور به استعفا شد\n\nhttpscdnisnaird20200319361598525jpg\n\n  \nانتهای پیام\n\n\nبه گزارش ایسنا امیر محسنی با بیان اینکه این لیگ به طور هفتگی در ایام نوروز ۹۹\nبا همکاری انجمن ورزش های الکترونیکی برگزار می شود گفت شهروندان می توانند جهت\nحضور در این لیگ ورزشی مهیج به سایت eligoir مراجعه کرده و ثبت نام کنند\n\nوی در پایان با درخواست از شهروندان برای ماندن در خانه و جدی گرفتن مقابله با\nشیوع ویروس کرونا افزود برگزاری لیگ بازی های الکترونیکی تولید و انتشار کلیپ\nهای ورزش درخانه و تمرین بازیهای خانوادگی از جمله فعالیت های سازمان ورزش\nشهرداری در نوروز ۹۹ است  \nانتهای پیام\n\n\nآرش فرهادیان در گفت و گو با ایسنا درباره آخرین وضعیت تمرینات تیم های ملی\nتکواندو با توجه به شیوع ویروس کرونا اظهار کرد اردوی تیم های ملی تکواندو تا\nروزهای پایانی سال ۹۸ برقرار بود و علاوه بر تکواندو اردوی تیم ملی پاراتکواندو\nنیز برقرار بود چرا که ما در رقابت های پارالمپیک نیز ۲ نماینده خواهیم داشت و\nباید برنامه ها را به نحوی جلو ببریم که در این شرایط خاص با رعایت نکات لازم\nبچه ها از آمادگی بدنی و تمرینات دور نشوند\n\nمدیر سازمان تیم های ملی تکواندو خاطرنشان کرد اردوها با رعایت تمامی موارد\nایمنی و بهداشتی برای جلوگیری از ابتلای بچه ها به ویروس کرونا انجام شد و مسائل\nمربوط به پیشگیری و حفاظت از بچه ها زیر نظر کادر پزشکی انجام شد با رعایت بسیار\nخوب و نظارتی که انجام شد خوشبختانه مشکلی برای هیچ کدام از تکواندوکاران و\nاعضای کادرهای فنی ایجاد نشد و اردوها بدون ایجاد مشکل به پایان رسید\n\nوی درباره برنامه های تیم های ملی در سال جدید تصریح کرد قصد داریم از ۱۵\nفروردین اردوی تیم های ملی را از سر بگیریم البته این احتمال وجود دارد که آرمین\nهادی پور و میرهاشم حسینی دو چهره المپیکی تکواندو زودتر از سایرین تمرینات خود\nرا آغاز کنند البته خواسته خودشان نیز این بود که زودتر تمریناتشان را از سر\nبگیرند که نظر کادر فنی نیز مساعد بود\n\nمدیر سازمان تیم های ملی افزود با آغاز اردوها قطعا تمامی نکات بهداشتی برای حفظ\nسلامتی بچه ها رعایت خواهد شد تا مشکلی برای نفراتمان بوجود نیاید و امیدوارم هر\nچه سریعتر با رعایت تمامی مردم شاهد ریشه کن شدن این ویروس باشیم\n\nفرهادیان در پایان گفت رقابت های قهرمانی آسیا و همچنین کسب سهمیه المپیک در بخش\nبانوان را در پیش داریم و پس از آن نیز رقابت های المپیک و پارالمپیک را پیش رو\nخواهیم داشت که امیدوارم با دعای خیر مردم بچه ها مزد زحماتشان را بگیرند و در\nاین رویدادها به بهترین نتایج برسند\n\nانتهای پیام\n\n\n"
     ]
    }
   ],
   "source": [
    "cc = df_50k.content\n",
    "\n",
    "xx = []\n",
    "for i in range(0, 3):\n",
    "    doc = cc[i]\n",
    "    print(delete_punctuations(doc))\n",
    "    # print(doc)"
   ]
  }
 ]
}