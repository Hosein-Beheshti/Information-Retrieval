{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd07ec3019a7cb8f5739f8b27d55fc304c51580b268fd2b34191f81e31e11772118",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import search\n",
    "import codecs\n",
    "import sys\n",
    "import string\n",
    "import math\n",
    "import heapq\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "prefixes = []\n",
    "postfixes = []\n",
    "verb_roots = []\n",
    "common_words = []\n",
    "plural_singular = []\n",
    "\n",
    "#######################################importing\n",
    "def read_dataset(path, name):\n",
    "    df = pd.read_excel(path + name)\n",
    "    return df\n",
    "\n",
    "def read_files():\n",
    "    path = 'files/'\n",
    "\n",
    "    #stop_words\n",
    "    name = 'stop_words.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        stop_words.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #prefixes\n",
    "    name = 'prefixes.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        prefixes.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #postfixes\n",
    "    name = 'postfixes.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        postfixes.append(line)\n",
    "    f.close()\n",
    "\n",
    "    # verb_roots\n",
    "    name = 'verb.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    count = 0\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        line = line.split('-')\n",
    "        verb_roots.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #common_words\n",
    "    name = 'common_words.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        common_words.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #plural_singular\n",
    "    name = 'plural_singular.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    count = 0\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        line = line.split('-')\n",
    "        plural_singular.append(line)\n",
    "    f.close()\n",
    "\n",
    "#######################################Normalization\n",
    "def delete_punctuations(doc):\n",
    "    punctuations = '؟،:؛!»«()[]*,{.}@!?،؛<>#$&!~\"\\|-_+'\n",
    "    edited_doc = doc.translate(str.maketrans('', '', punctuations))\n",
    "    edited_doc = edited_doc.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return edited_doc\n",
    "\n",
    "def delete_stopWords(doc):\n",
    "    edited_doc = doc\n",
    "    for s in stop_words:\n",
    "        my_regex = r\"\\b\"+s+r\"\\b\"\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def delete_highFrequencyWords(inverted_indexes):\n",
    "    df_temp = df.copy()\n",
    "    for i in inverted_indexes:\n",
    "        if len(i[1])/len(df_temp) > 0.8:\n",
    "            inverted_indexes.remove(i)\n",
    "    return inverted_indexes\n",
    "\n",
    "def delete_postfixes(doc):\n",
    "    edited_doc = doc\n",
    "    for p in postfixes:\n",
    "        my_regex = p + r\"\\b\"\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def delete_prefixes(doc):\n",
    "    edited_doc = doc\n",
    "    for p in postfixes:\n",
    "        my_regex = r\"\\b\" + p\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def replaceWithRoot(tokens):   \n",
    "    for i in range(0, len(tokens)):\n",
    "        for root in verb_roots:\n",
    "            if search(root[0], tokens[i]) or search(root[1], tokens[i]):\n",
    "                common = False\n",
    "                for c in common_words:\n",
    "                    if c == tokens[i]:\n",
    "                        common = True\n",
    "                        # print(\"common\")\n",
    "                        break\n",
    "                if common == False:\n",
    "                    # print(\"root\")\n",
    "                    tokens[i] = tokens[i].replace(tokens[i], root[2])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def replaceArabicWords(doc):\n",
    "    doc = doc.replace('ك', 'ک')\n",
    "    doc = doc.replace('ئ', 'ی')\n",
    "    doc = doc.replace('ي', 'ی')\n",
    "    doc = doc.replace('ؤ', 'و')\n",
    "    doc = doc.replace('هٔ', 'ه')\n",
    "    doc = doc.replace('ة', 'ه')\n",
    "    doc = doc.replace('آ', 'ا')\n",
    "    doc = doc.replace('أ', 'ا')\n",
    "    doc = doc.replace('إ', 'ا')\n",
    "    return doc\n",
    "\n",
    "def pluralToSingular(tokens):\n",
    "    for i in range(0, len(tokens)):\n",
    "        for ps in plural_singular:\n",
    "            if search(ps[0], tokens[i]):\n",
    "                tokens[i] = tokens[i].replace(tokens[i], ps[1])\n",
    "    return tokens\n",
    "\n",
    "#######################################tokenization\n",
    "def tokenize(df):\n",
    "    content = df.content\n",
    "\n",
    "    tokens = []\n",
    "    for i in range(0, content.size):\n",
    "        doc = content[i]\n",
    "        # 68000 tokens\n",
    "        doc = delete_punctuations(doc)\n",
    "        # 50000 tokens\n",
    "        doc = delete_stopWords(doc)\n",
    "\n",
    "        doc = replaceArabicWords(doc)\n",
    "\n",
    "        doc = delete_postfixes(doc)\n",
    "\n",
    "        doc = delete_prefixes(doc)\n",
    "        \n",
    "\n",
    "        tokenized_doc = doc.split()\n",
    "        tokenized_doc = replaceWithRoot(tokenized_doc)\n",
    "        tokenized_doc = pluralToSingular(tokenized_doc)\n",
    "\n",
    "        for token in tokenized_doc:\n",
    "            if len(token) > 0:\n",
    "                temp = []\n",
    "                temp.append(token)\n",
    "                temp.append(df.id[i])\n",
    "                tokens.append(temp)\n",
    "    tokens.sort()\n",
    "    return tokens\n",
    "\n",
    "#######################################create inverted index\n",
    "def create_inverted_indexes(tokens):\n",
    "    inverted_indexes = []\n",
    "    doc_temp = []\n",
    "    token_temp = \"\"\n",
    "    for token in tokens:\n",
    "        if token[0] == token_temp:\n",
    "            doc_temp.append(token[1])\n",
    "        else:\n",
    "            temp = []\n",
    "            temp.append(token_temp)\n",
    "\n",
    "            (unique, counts) = np.unique(doc_temp, return_counts=True)\n",
    "            # frequencies = np.array((unique, counts), dtype='i4,f4').T.view(np.recarray)       \n",
    "            frequencies = []      \n",
    "            for i in range(len(unique)):\n",
    "                temp1 = []\n",
    "                temp1.append(unique[i])\n",
    "                temp1.append(counts[i])\n",
    "                frequencies.append(temp1)\n",
    "            temp.append(frequencies)\n",
    "            inverted_indexes.append(temp)\n",
    "            doc_temp = []\n",
    "\n",
    "        token_temp = token[0]\n",
    "        # if len(doc_temp) > 0 and token[1] != doc_temp[-1]:\n",
    "        doc_temp.append(token[1])\n",
    "    \n",
    "    inverted_indexes = delete_highFrequencyWords(inverted_indexes)\n",
    "    return inverted_indexes\n",
    "\n",
    "#######################################calculate weights\n",
    "def calculate_tfidf(df, inverted_indexes):\n",
    "    doc_length = np.zeros(len(df)+1)\n",
    "    len_df = len(df)\n",
    "    for i in range(len(inverted_indexes)):\n",
    "        if len(inverted_indexes[i][1]) > 0:\n",
    "            # print(weighted_inverted_index[i])\n",
    "            # print(len(inverted_indexes[i][1]))\n",
    "            idf = math.log(len_df/len(inverted_indexes[i][1]), 10)\n",
    "            inverted_indexes[i].append(idf)\n",
    "            # print(\"idf = \", idf)\n",
    "            for j in range(len(inverted_indexes[i][1])):\n",
    "                tf = 1 + math.log(inverted_indexes[i][1][j][1], 10)\n",
    "                # print(\"tf = \", tf)\n",
    "                w = tf*idf\n",
    "                # print(\"w : \",w)\n",
    "                inverted_indexes[i][1][j][1] = w\n",
    "                doc_length[inverted_indexes[i][1][j][0]] = doc_length[inverted_indexes[i][1][j][0]] + np.power(w, 2)\n",
    "                \n",
    "    doc_length =  np.sqrt(doc_length)\n",
    "    return inverted_indexes, doc_length\n",
    "\n",
    "#######################################create champion list\n",
    "def create_championList(inverted_indexes, r):\n",
    "    limit = r\n",
    "    championList = []\n",
    "    for i in range(len(inverted_indexes)):\n",
    "        temp = []\n",
    "        temp.append(inverted_indexes[i][0])\n",
    "        temp_postingList = list(inverted_indexes[i][1])\n",
    "        temp_postingList.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        temp1 = []\n",
    "        r = limit\n",
    "        if r > len(temp_postingList):\n",
    "            r = len(temp_postingList)\n",
    "        for j in range(r):\n",
    "            # print(temp_postingList[j])\n",
    "            temp1.append(temp_postingList[j])\n",
    "        temp.append(temp1)\n",
    "        if len(inverted_indexes[i]) == 3:\n",
    "            temp.append(inverted_indexes[i][2])\n",
    "        championList.append(temp)\n",
    "    return championList\n",
    "\n",
    "#######################################Searching\n",
    "def search_token(inverted_indexes, token_name):\n",
    "    for token in inverted_indexes:\n",
    "        if token[0] == token_name:\n",
    "            return token[1], token[2]\n",
    "    \n",
    "    print(\"this token not exist in our database\")\n",
    "    return -1, -1\n",
    "\n",
    "def print_res(df, res):\n",
    "    p_res = pd.DataFrame()\n",
    "\n",
    "    p_res = pd.merge(df, res, on=['id'], how='inner')\n",
    "    our_res = p_res[['id', 'rank', 'url']]\n",
    "    our_res = our_res.sort_values([\"rank\"], ascending=False)\n",
    "\n",
    "    print(our_res)\n",
    "    \n",
    "\n",
    "def binary_search(df, inverted_indexes, query):\n",
    "    res = []\n",
    "    docs_id = pd.DataFrame()\n",
    "\n",
    "\n",
    "    query = delete_punctuations(query)\n",
    "    query = delete_stopWords(query)\n",
    "    query = delete_postfixes(query)\n",
    "    query = delete_prefixes(query)\n",
    "    query = replaceArabicWords(query)\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    tokenized_query = replaceWithRoot(tokenized_query)\n",
    "    tokenized_query = pluralToSingular(tokenized_query)\n",
    "\n",
    "    if len(tokenized_query) == 0:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return\n",
    "\n",
    "\n",
    "    for i in range (0 ,len(tokenized_query)):\n",
    "        docs, idf = search_token(inverted_indexes, tokenized_query[i])\n",
    "        if docs != -1:\n",
    "            temp_docs_id = pd.DataFrame(docs)\n",
    "            docs_id = pd.DataFrame(docs_id.append(temp_docs_id))\n",
    "\n",
    "\n",
    "    docs_id = docs_id[0].value_counts().reset_index()\n",
    "    docs_id.columns = ['id', 'rank']\n",
    "    print_res(df, docs_id)\n",
    "\n",
    "\n",
    "def ranked_search(df, inverted_indexes, query, doc_length, k, using_heap):\n",
    "    res = []\n",
    "    docs_id = pd.DataFrame()\n",
    "    \n",
    "    query = delete_punctuations(query)\n",
    "    query = delete_stopWords(query)\n",
    "    query = delete_postfixes(query)\n",
    "    query = delete_prefixes(query)\n",
    "    query = replaceArabicWords(query)\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    tokenized_query = replaceWithRoot(tokenized_query)\n",
    "    tokenized_query = pluralToSingular(tokenized_query)\n",
    "\n",
    "    if len(tokenized_query) == 0:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return\n",
    "\n",
    "    (unique, counts) = np.unique(tokenized_query, return_counts=True)\n",
    "\n",
    "    tokenized_query = np.asarray((unique, counts)).T\n",
    "    # print(tokenized_query)\n",
    "\n",
    "    doc_score_temp = []\n",
    "    for i in range (0 ,len(tokenized_query)):\n",
    "        tf_query = 1 + math.log(int(tokenized_query[i][1]), 10)\n",
    "        # print(tokenized_query[i][0])\n",
    "        docs, idf = search_token(inverted_indexes, tokenized_query[i][0])\n",
    "\n",
    "\n",
    "        if docs != -1:\n",
    "            # print(docs)\n",
    "            # print(idf)\n",
    "            w_termInQuery = tf_query*idf\n",
    "            # print(\"weight term in q: \", tf_query, idf, w_termInQuery)\n",
    "\n",
    "            for i in range(len(docs)):\n",
    "                temp = []\n",
    "                temp.append(docs[i][0])\n",
    "                # print(\"docs[i][1]: \",docs[i][1])\n",
    "                temp.append(w_termInQuery*docs[i][1])\n",
    "                doc_score_temp.append(temp)\n",
    "\n",
    "    doc_score_temp.sort()\n",
    "    doc_score = []\n",
    "    doc_score.append(doc_score_temp[0])\n",
    "    for i in range(1, len(doc_score_temp)):\n",
    "        if doc_score[len(doc_score)-1][0] == doc_score_temp[i][0]:\n",
    "            doc_score[len(doc_score)-1][1] = doc_score[len(doc_score)-1][1] + doc_score_temp[i][1]\n",
    "        else:\n",
    "            doc_score.append(doc_score_temp[i])\n",
    "    \n",
    "    for i in range(len(doc_score)):\n",
    "        doc_score[i][1] = doc_score[i][1] / doc_length[doc_score[i][0]]\n",
    "\n",
    "    res = []\n",
    "    if using_heap == False:\n",
    "        doc_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        if len(doc_score) > k:\n",
    "            for i in range (k):\n",
    "                res.append(doc_score[i])\n",
    "        else:\n",
    "            res = doc_score\n",
    "    if using_heap == True:\n",
    "        hp = []\n",
    "        for e in doc_score:\n",
    "            hp.append((e[1], e[0]))\n",
    "\n",
    "        nlargest = heapq.nlargest(k, hp)\n",
    "        k_high_ranked = []\n",
    "        for i in nlargest:\n",
    "            k_high_ranked.append([i[1], i[0]])\n",
    "\n",
    "        res = k_high_ranked\n",
    "\n",
    "    docs_id = pd.DataFrame(res, columns=['id', 'rank'])\n",
    "    print_res(df, docs_id)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_dataset(\"datasets/\", \"IR_Spring2021_ph12_7k.xlsx\")\n",
    "read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_indexes = create_inverted_indexes(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_indexes, doc_length = calculate_tfidf(df, inverted_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "championList = create_championList(inverted_indexes, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"کشف یک میدان گازی در جنوب ایران\"\n",
    "# query_processing(df, inverted_indexes, query)\n",
    "\n",
    "doc_score = ranked_search(df, championList, query, doc_length, k = 10, using_heap = True)\n",
    "doc_score = ranked_search(df, inverted_indexes, query, doc_length, k = 10, using_heap = True)\n",
    "\n",
    "\n",
    "binary_search(df, inverted_indexes, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inverted_indexes[1][1])\n",
    "temp_postingList = list(inverted_indexes[1][1])\n",
    "temp_postingList.sort(key=lambda x: x[1], reverse=True)\n",
    "print(temp_postingList)\n",
    "print(inverted_indexes[1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(championList[5000])\n",
    "# print(inverted_indexes[5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"؟?#@است،؛\"\n",
    "x = delete_punctuations(s)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}