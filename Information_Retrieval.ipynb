{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd07ec3019a7cb8f5739f8b27d55fc304c51580b268fd2b34191f81e31e11772118",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import search\n",
    "import codecs\n",
    "import sys"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    \"از\",\n",
    "    \"این\",\n",
    "    \"آن\",\n",
    "    \"به\",\n",
    "    \"با\",\n",
    "    \"بر\",\n",
    "    \"برای\",\n",
    "    \"پس\",\n",
    "    \"تا\",\n",
    "    \"در\",\n",
    "    \"را\",\n",
    "    \"که\",\n",
    "    \"و\",\n",
    "]\n",
    "\n",
    "prefixes = [\n",
    "    \"با\",\n",
    "    \"بی\",\n",
    "    \"نا\"\n",
    "]\n",
    "\n",
    "postfixes = [\n",
    "    \"تر\",\n",
    "    \"ترین\",\n",
    "    \"ات\",\n",
    "    \"ها\"\n",
    "]\n",
    "\n",
    "verb_roots = [\n",
    "    \"گفت\",\n",
    "    \"رفت\",\n",
    "    \"شد\"\n",
    "]\n",
    "\n",
    "\n",
    "def read_dataset(path, name):\n",
    "    df = pd.read_excel(path + name)\n",
    "    return df\n",
    "\n",
    "def delete_punctuations(doc):\n",
    "    punctuations = '،:؛؟!»«()[]\"*,{.}@!?'\n",
    "    edited_doc = doc.translate(str.maketrans('', '', punctuations))\n",
    "    return edited_doc\n",
    "\n",
    "def delete_stopWords(doc):\n",
    "    edited_doc = doc\n",
    "    for s in stop_words:\n",
    "        my_regex = r\"\\b\"+s+r\"\\b\"\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def delete_highFrequencyWords(inverted_indexes):\n",
    "    df_temp = df.copy()\n",
    "    for i in inverted_indexes:\n",
    "        if len(i[1])/len(df_temp) > 0.5:\n",
    "            print(i[0])\n",
    "            inverted_indexes.remove(i)\n",
    "    return inverted_indexes\n",
    "\n",
    "def delete_postfixes(doc):\n",
    "    edited_doc = doc\n",
    "    for p in postfixes:\n",
    "        my_regex = p + r\"\\b\"\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def delete_prefixes(doc):\n",
    "    edited_doc = doc\n",
    "    for p in postfixes:\n",
    "        my_regex = r\"\\b\" + p\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def replaceWithRoot(tokens):   \n",
    "    for i in range(0, len(tokens)):\n",
    "        for root in verb_roots:\n",
    "            if search(root, tokens[i]):\n",
    "                print(\"root\")\n",
    "                tokens[i] = tokens[i].replace(tokens[i], root)\n",
    "    return tokens\n",
    "\n",
    "def replaceArabicWords(doc):\n",
    "    doc = doc.replace('ك', 'ک')\n",
    "    doc = doc.replace('ئ', 'ی')\n",
    "    doc = doc.replace('ي', 'ی')\n",
    "    doc = doc.replace('ؤ', 'و')\n",
    "    doc = doc.replace('هٔ', 'ه')\n",
    "    doc = doc.replace('ة', 'ه')\n",
    "    doc = doc.replace('آ', 'ا')\n",
    "    doc = doc.replace('أ', 'ا')\n",
    "    doc = doc.replace('إ', 'ا')\n",
    "    return doc\n",
    "\n",
    "def tokenize(df):\n",
    "    content = df.content\n",
    "\n",
    "    tokens = []\n",
    "    for i in range(0, content.size):\n",
    "        doc = content[i]\n",
    "        # 68000 tokens\n",
    "        doc = delete_punctuations(doc)\n",
    "        # 50000 tokens\n",
    "        # doc = delete_stopWords(doc)\n",
    "\n",
    "        # doc = delete_postfixes(doc)\n",
    "\n",
    "        # doc = delete_prefixes(doc)\n",
    "\n",
    "        # doc = replaceArabicWords(doc)\n",
    "        \n",
    "        tokenized_doc = doc.split()\n",
    "        # tokenized_doc = replaceWithRoot(tokenized_doc)\n",
    "\n",
    "        for token in tokenized_doc:\n",
    "            temp = []\n",
    "            temp.append(token)\n",
    "            temp.append(df.id[i])\n",
    "            tokens.append(temp)\n",
    "    tokens.sort()\n",
    "    return tokens\n",
    "\n",
    "def create_inverted_indexes(tokens):\n",
    "    inverted_indexes = []\n",
    "    doc_temp = []\n",
    "    token_temp = \"\"\n",
    "    for token in tokens:\n",
    "        if token[0] == token_temp:\n",
    "            doc_temp.append(token[1])\n",
    "        else:\n",
    "            temp = []\n",
    "            temp.append(token_temp)\n",
    "            doc_temp = set(doc_temp)\n",
    "            temp.append(doc_temp)\n",
    "            inverted_indexes.append(temp)\n",
    "            doc_temp = []\n",
    "\n",
    "        token_temp = token[0]\n",
    "        # if len(doc_temp) > 0 and token[1] != doc_temp[-1]:\n",
    "        doc_temp.append(token[1])\n",
    "    \n",
    "    # inverted_indexes = delete_highFrequencyWords(inverted_indexes)\n",
    "    return inverted_indexes\n",
    "\n",
    "def search_token(inverted_indexes, token_name):\n",
    "    for token in inverted_indexes:\n",
    "        if token[0] == token_name:\n",
    "            return token[1]\n",
    "    print(\"this token not exist in our database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_dataset(\"datasets/\", \"IR_Spring2021_ph12_7k.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'ptint' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-13d360fdeb76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mptint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# inverted_indexes = create_inverted_indexes(tokens)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ptint' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(df)\n",
    "inverted_indexes = create_inverted_indexes(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2192769\n"
     ]
    }
   ],
   "source": [
    "search_token(inverted_indexes, \"اجتماعی\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{27}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# print(len(inverted_indexes))\n",
    " print(inverted_indexes)\n",
    "print(len(tokens))\n",
    "\n",
    "# for i in range(5000, 5100):\n",
    "#     print(inverted_indexes[i][0])\n",
    "#     print(inverted_indexes[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "تراوت تر عالی ترین خوبتر برتر آبها\nتراوت  عالی  خوب بر آب\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tweet = \"I am tired! I like fruit...and milk\"\n",
    "s = \"هی.پسر؟ چطوری تو. من، عالیی!!!!!\"\n",
    "s1 = \"تراوت تر عالی ترین خوبتر برتر آبها\"\n",
    "\n",
    "\n",
    "# clean = re.sub(r\"\"\"\n",
    "#                [\"،:؛؟!»«()[*,{.}@!? ]+  # Accept one or more copies of punctuation\n",
    "#                \\ *           # plus zero or more copies of a space,\n",
    "#                \"\"\",\n",
    "#                \" \",          # and replace it with a single space\n",
    "#                s1, flags=re.VERBOSE)\n",
    "\n",
    "postfix = [\n",
    "    \"تر\",\n",
    "    \"ترین\",\n",
    "    \"ات\",\n",
    "    \"ها\"\n",
    "]\n",
    "\n",
    "postfix = [\n",
    "    \"تر\",\n",
    "    \"ترین\",\n",
    "    \"ات\",\n",
    "    \"ها\"\n",
    "]\n",
    "\n",
    "clean = s1\n",
    "for p in postfix:\n",
    "    my_regex = p + r\"\\b\"\n",
    "    clean = re.sub(my_regex , \"\", clean)\n",
    "\n",
    "print(s1 + \"\\n\" + clean)\n",
    "\n",
    "p = '،:؛؟!»«()[]\"*,{.}@!?'\n",
    "s = \"هی.پسر؟ چطوری تو. من، عالیی!!!!!\"\n",
    "s1 = \"بیدارترین،خوب تر،باغ ها\"\n",
    "\n",
    "\n",
    "\n",
    "# x = s.translate(str.maketrans('', '', p))\n",
    "# print(x)\n",
    "\n",
    "\n",
    "# for token in doc:\n",
    "#     for p in punctuations:\n",
    "#         if token == p:\n",
    "#             token.replace(\"geeks\", \"Geeks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "آمدن\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer= PorterStemmer()\n",
    "input_str = \"آمدن\"\n",
    "# for W in input_str:\n",
    "print(stemmer.stem(input_str))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "true\ntrue\n['یس', 'رفت', 'می روی', 'گفت']\n"
     ]
    }
   ],
   "source": [
    "from re import search\n",
    "tokens = [\"یس\",\"نمی رفتن\", \"می روی\", \"میگفت\"]\n",
    "verb_roots = [\"رفت\",\n",
    "        \"گفت\"]\n",
    "# my_regex = r\"[a-z0-9]+$\" + p + r\"\"\n",
    "# clean = re.sub(my_regex , p, clean)\n",
    "# print(clean)  \n",
    "\n",
    "for i in range(0, len(tokens)):\n",
    "    for root in verb_roots:\n",
    "        if search(root, tokens[i]):\n",
    "            print(\"true\")\n",
    "            tokens[i] = tokens[i].replace(tokens[i], root)\n",
    "            \n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ه سلام ؤ\n"
     ]
    }
   ],
   "source": [
    "x  = \"هٔ سلام ؤ\"\n",
    "x = x.replace('هٔ', 'ه')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}