{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "interpreter": {
   "hash": "7ec3019a7cb8f5739f8b27d55fc304c51580b268fd2b34191f81e31e11772118"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Implementing an Information Retrieval Engine\n",
    "in this project we are going to implement an information retrieval engine with this steps.\n",
    "* tokenization with normalization\n",
    "* creating inverted index and posting lists\n",
    "* binary searching\n",
    "* ranking search using tfidf weighting\n",
    "### some ideas to increase speed:\n",
    "* champion list\n",
    "* clustering (K-Means)\n",
    "### more options:\n",
    "* using labeled datasets to predict label of new datasets and adding feature of classification searching"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import search\n",
    "import codecs\n",
    "import sys\n",
    "import string\n",
    "import math\n",
    "import heapq\n",
    "import pickle"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "prefixes = []\n",
    "postfixes = []\n",
    "verb_roots = []\n",
    "common_words = []\n",
    "plural_singular = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################importing\n",
    "def read_dataset(path, name):\n",
    "    df = pd.read_excel(path + name)\n",
    "    return df\n",
    "\n",
    "def read_files():\n",
    "    path = 'files/'\n",
    "\n",
    "    #stop_words\n",
    "    name = 'stop_words.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        stop_words.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #prefixes\n",
    "    name = 'prefixes.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        prefixes.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #postfixes\n",
    "    name = 'postfixes.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        postfixes.append(line)\n",
    "    f.close()\n",
    "\n",
    "    # verb_roots\n",
    "    name = 'verb.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    count = 0\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        line = line.split('-')\n",
    "        verb_roots.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #common_words\n",
    "    name = 'common_words.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        common_words.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #plural_singular\n",
    "    name = 'plural_singular.txt'\n",
    "    f = open(path + name, 'r', encoding='utf-8')\n",
    "    Lines = f.read().splitlines()\n",
    "    count = 0\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        line = line.split('-')\n",
    "        plural_singular.append(line)\n",
    "    f.close()\n",
    "\n",
    "#######################################Normalization\n",
    "def delete_punctuations(doc):\n",
    "    punctuations = '؟،:؛!»«()[]*,{.}@!?،؛<>#$&!~\"\\|-_+'\n",
    "    edited_doc = doc.translate(str.maketrans('', '', punctuations))\n",
    "    edited_doc = edited_doc.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return edited_doc\n",
    "\n",
    "def delete_stopWords(doc):\n",
    "    edited_doc = doc\n",
    "    for s in stop_words:\n",
    "        my_regex = r\"\\b\"+s+r\"\\b\"\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def delete_highFrequencyWords(inverted_indexes):\n",
    "    for i in inverted_indexes:\n",
    "        if len(i[1])/len(df) > 0.8:\n",
    "            inverted_indexes.remove(i)\n",
    "    return inverted_indexes\n",
    "\n",
    "def delete_postfixes(doc):\n",
    "    edited_doc = doc\n",
    "    for p in postfixes:\n",
    "        my_regex = p + r\"\\b\"\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def delete_prefixes(doc):\n",
    "    edited_doc = doc\n",
    "    for p in postfixes:\n",
    "        my_regex = r\"\\b\" + p\n",
    "        edited_doc = re.sub(my_regex , \"\", edited_doc)\n",
    "    return edited_doc\n",
    "\n",
    "def replaceWithRoot(tokens): \n",
    "    for i in range(0, len(tokens)):\n",
    "        # print(verb_roots)\n",
    "        for root in verb_roots:\n",
    "            if search(root[0], tokens[i]) or search(root[1], tokens[i]):\n",
    "                common = False\n",
    "                for c in common_words:\n",
    "                    if c == tokens[i]:\n",
    "                        common = True\n",
    "                        # print(\"common\")\n",
    "                        break\n",
    "                if common == False:\n",
    "                    my_regex = r\"\\b^\"+root[1]+r\"\\.*\"\n",
    "                    x = re.findall(my_regex, tokens[i])\n",
    " \n",
    "                    if len(x) == 0:  \n",
    "                        my_regex = r\"\\b...*\"+root[1]+r\"\\b\"\n",
    "                        x1 = re.findall(my_regex, tokens[i])\n",
    "                        if len(x1)==0:         \n",
    "                            # print(\"root\")\n",
    "                            tokens[i] = tokens[i].replace(tokens[i], root[2])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def replaceArabicWords(doc):\n",
    "    doc = doc.replace('ك', 'ک')\n",
    "    doc = doc.replace('ئ', 'ی')\n",
    "    doc = doc.replace('ي', 'ی')\n",
    "    doc = doc.replace('ؤ', 'و')\n",
    "    doc = doc.replace('هٔ', 'ه')\n",
    "    doc = doc.replace('ة', 'ه')\n",
    "    doc = doc.replace('آ', 'ا')\n",
    "    doc = doc.replace('أ', 'ا')\n",
    "    doc = doc.replace('إ', 'ا')\n",
    "    return doc\n",
    "\n",
    "def pluralToSingular(tokens):\n",
    "    for i in range(0, len(tokens)):\n",
    "        for ps in plural_singular:\n",
    "            if search(ps[0], tokens[i]):\n",
    "                tokens[i] = tokens[i].replace(tokens[i], ps[1])\n",
    "    return tokens\n",
    "\n",
    "#######################################tokenization\n",
    "def tokenize(df):\n",
    "    content = df.content\n",
    "\n",
    "    tokens = []\n",
    "    for i in range(0, content.size):\n",
    "        doc = content[i]\n",
    "        # 68000 tokens\n",
    "        doc = delete_punctuations(doc)\n",
    "        # 50000 tokens\n",
    "        doc = delete_stopWords(doc)\n",
    "\n",
    "        doc = replaceArabicWords(doc)\n",
    "\n",
    "        doc = delete_postfixes(doc)\n",
    "\n",
    "        doc = delete_prefixes(doc)\n",
    "        \n",
    "\n",
    "        tokenized_doc = doc.split()\n",
    "        tokenized_doc = replaceWithRoot(tokenized_doc)\n",
    "        tokenized_doc = pluralToSingular(tokenized_doc)\n",
    "\n",
    "        for token in tokenized_doc:\n",
    "            if len(token) > 0:\n",
    "                temp = []\n",
    "                temp.append(token)\n",
    "                temp.append(df.id[i])\n",
    "                tokens.append(temp)\n",
    "    # tokens.sort()\n",
    "    return tokens\n",
    "\n",
    "#######################################create inverted index\n",
    "def create_inverted_indexes(tokens):\n",
    "    tokens.sort()\n",
    "    inverted_indexes = []\n",
    "    doc_temp = []\n",
    "    token_temp = \"\"\n",
    "    for token in tokens:\n",
    "        if token[0] == token_temp:\n",
    "            doc_temp.append(token[1])\n",
    "        else:\n",
    "            if len(token[0])>0:\n",
    "                temp = []\n",
    "                temp.append(token_temp)\n",
    "\n",
    "                (unique, counts) = np.unique(doc_temp, return_counts=True)\n",
    "                # frequencies = np.array((unique, counts), dtype='i4,f4').T.view(np.recarray)       \n",
    "                frequencies = []      \n",
    "                for i in range(len(unique)):\n",
    "                    temp1 = []\n",
    "                    temp1.append(unique[i])\n",
    "                    temp1.append(counts[i])\n",
    "                    frequencies.append(temp1)\n",
    "                temp.append(frequencies)\n",
    "                inverted_indexes.append(temp)\n",
    "                doc_temp = []\n",
    "                doc_temp.append(token[1])\n",
    "\n",
    "        token_temp = token[0]\n",
    "        # if len(doc_temp) > 0 and token[1] != doc_temp[-1]:\n",
    "    \n",
    "    inverted_indexes = delete_highFrequencyWords(inverted_indexes)\n",
    "    return inverted_indexes\n",
    "\n",
    "#######################################calculate weights\n",
    "def calculate_tfidf(df, inverted_indexes):\n",
    "    doc_length = np.zeros(len(df)+1)\n",
    "    len_df = len(df)\n",
    "    for i in range(len(inverted_indexes)):\n",
    "        if len(inverted_indexes[i][1]) > 0:\n",
    "            # print(weighted_inverted_index[i])\n",
    "            # print(len(inverted_indexes[i][1]))\n",
    "            idf = math.log(len_df/len(inverted_indexes[i][1]), 10)\n",
    "            inverted_indexes[i].append(idf)\n",
    "            # print(\"idf = \", idf)\n",
    "            for j in range(len(inverted_indexes[i][1])):\n",
    "                tf = 1 + math.log(inverted_indexes[i][1][j][1], 10)\n",
    "                # print(\"tf = \", tf)\n",
    "                w = tf*idf\n",
    "                # print(\"w : \",w)\n",
    "                inverted_indexes[i][1][j][1] = w\n",
    "                # if inverted_indexes[i][1][j][0] == 6545:\n",
    "                #     print(\"6545: \", inverted_indexes[i][1][j])\n",
    "                #     print(\"6545: \", inverted_indexes[i][0])\n",
    "                # if inverted_indexes[i][1][j][0] == 6550:\n",
    "                #     print(\"6550: \", inverted_indexes[i][1][j])\n",
    "                #     print(\"6550: \", inverted_indexes[i][0])\n",
    "\n",
    "\n",
    "                doc_length[inverted_indexes[i][1][j][0]] = doc_length[inverted_indexes[i][1][j][0]] + np.power(w, 2)\n",
    "                \n",
    "    doc_length =  np.sqrt(doc_length)\n",
    "    return inverted_indexes, doc_length\n",
    "\n",
    "#######################################create champion list\n",
    "def create_championList(inverted_indexes, r):\n",
    "    limit = r\n",
    "    championList = []\n",
    "    for i in range(len(inverted_indexes)):\n",
    "        temp = []\n",
    "        temp.append(inverted_indexes[i][0])\n",
    "        temp_postingList = list(inverted_indexes[i][1])\n",
    "        temp_postingList.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        temp1 = []\n",
    "        r = limit\n",
    "        if r > len(temp_postingList):\n",
    "            r = len(temp_postingList)\n",
    "        for j in range(r):\n",
    "            # print(temp_postingList[j])\n",
    "            temp1.append(temp_postingList[j])\n",
    "        temp.append(temp1)\n",
    "        if len(inverted_indexes[i]) == 3:\n",
    "            temp.append(inverted_indexes[i][2])\n",
    "        championList.append(temp)\n",
    "    return championList\n",
    "\n",
    "#######################################Searching\n",
    "def search_token(inverted_indexes, token_name):\n",
    "    # for token in inverted_indexes:\n",
    "    #     if token[0] == token_name:\n",
    "    #         # print(\"token*****: \", token[0])\n",
    "    #         return token[1], token[2]\n",
    "    if token_name in inverted_indexes:\n",
    "        return inverted_indexes[token_name][0], inverted_indexes[token_name][1]\n",
    "    # print(\"this token not exist in our database\")\n",
    "    return -1, -1\n",
    "\n",
    "def print_res(df, res):\n",
    "    p_res = pd.DataFrame()\n",
    "\n",
    "    p_res = pd.merge(df, res, on=['id'], how='inner')\n",
    "    if 'topic' in df:\n",
    "        our_res = p_res[['id', 'rank', 'topic', 'url']]\n",
    "    else:\n",
    "        our_res = p_res[['id', 'rank', 'url']]\n",
    "    our_res = our_res.sort_values([\"rank\"], ascending=False)\n",
    "\n",
    "    return(our_res)\n",
    "    \n",
    "\n",
    "def binary_search(df, inverted_indexes, query):\n",
    "    res = []\n",
    "    docs_id = pd.DataFrame()\n",
    "\n",
    "\n",
    "    query = delete_punctuations(query)\n",
    "    query = delete_stopWords(query)\n",
    "    query = delete_postfixes(query)\n",
    "    query = delete_prefixes(query)\n",
    "    query = replaceArabicWords(query)\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    tokenized_query = replaceWithRoot(tokenized_query)\n",
    "    tokenized_query = pluralToSingular(tokenized_query)\n",
    "\n",
    "    if len(tokenized_query) == 0:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return\n",
    "\n",
    "\n",
    "    for i in range (0 ,len(tokenized_query)):\n",
    "        docs, idf = search_token(inverted_indexes, tokenized_query[i])\n",
    "        if docs != -1:\n",
    "            temp_docs_id = pd.DataFrame(docs)\n",
    "            docs_id = pd.DataFrame(docs_id.append(temp_docs_id))\n",
    "\n",
    "    if len(docs_id) < 1:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return -1\n",
    "\n",
    "    docs_id = docs_id[0].value_counts().reset_index()\n",
    "    docs_id.columns = ['id', 'rank']\n",
    "    df_res = print_res(df, docs_id)\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def ranked_search(df, inverted_indexes, query, doc_length, k, using_heap):\n",
    "    res = []\n",
    "    docs_id = pd.DataFrame()\n",
    "    try:\n",
    "        using_class = False\n",
    "        pattern = \"cat\\:(.*?)\\ \"\n",
    "        topic = re.search(pattern, query).group(1)\n",
    "        if len(topic) > 0:\n",
    "            print(topic)\n",
    "            using_class = True\n",
    "    except AttributeError:\n",
    "        topic = re.search(pattern, query)\n",
    "    \n",
    "    query = delete_punctuations(query)\n",
    "    query = delete_stopWords(query)\n",
    "    query = delete_postfixes(query)\n",
    "    query = delete_prefixes(query)\n",
    "    query = replaceArabicWords(query)\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    tokenized_query = replaceWithRoot(tokenized_query)\n",
    "    tokenized_query = pluralToSingular(tokenized_query)\n",
    "\n",
    "    if len(tokenized_query) == 0:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return -1\n",
    "\n",
    "    (unique, counts) = np.unique(tokenized_query, return_counts=True)\n",
    "\n",
    "    tokenized_query = np.asarray((unique, counts)).T\n",
    "    # print(tokenized_query)\n",
    "\n",
    "    doc_score_temp = []\n",
    "    for i in range (0 ,len(tokenized_query)):\n",
    "        tf_query = 1 + math.log(int(tokenized_query[i][1]), 10)\n",
    "        # print(tokenized_query[i][0])\n",
    "        docs, idf = search_token(inverted_indexes, tokenized_query[i][0])\n",
    "        # print(docs)\n",
    "\n",
    "\n",
    "        if docs != -1:\n",
    "            # print(docs)\n",
    "            # print(idf)\n",
    "            w_termInQuery = tf_query*idf\n",
    "            # print(\"weight term in q: \", tf_query, idf, w_termInQuery)\n",
    "\n",
    "            for i in range(len(docs)):\n",
    "                temp = []\n",
    "                if using_class == True:\n",
    "                    if dict_topics[docs[i][0]] == topic:\n",
    "                        temp.append(docs[i][0])\n",
    "                        # print(\"docs[i][1]: \",docs[i][1])\n",
    "                        temp.append(w_termInQuery*docs[i][1])\n",
    "                        doc_score_temp.append([docs[i][0], w_termInQuery*docs[i][1]])\n",
    "                else:\n",
    "                    temp.append(docs[i][0])\n",
    "                    # print(\"docs[i][1]: \",docs[i][1])\n",
    "                    temp.append(w_termInQuery*docs[i][1])\n",
    "                    doc_score_temp.append([docs[i][0], w_termInQuery*docs[i][1]])\n",
    "\n",
    "    if len(doc_score_temp) < 1:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return -1\n",
    "    doc_score_temp.sort()\n",
    "    doc_score = []\n",
    "    doc_score.append(doc_score_temp[0])\n",
    "    for i in range(1, len(doc_score_temp)):\n",
    "        if doc_score[len(doc_score)-1][0] == doc_score_temp[i][0]:\n",
    "            doc_score[len(doc_score)-1][1] = doc_score[len(doc_score)-1][1] + doc_score_temp[i][1]\n",
    "        else:\n",
    "            doc_score.append(doc_score_temp[i])\n",
    "    \n",
    "    # for i in range(len(doc_score)):\n",
    "    #     doc_score[i][1] = doc_score[i][1] / doc_length[doc_score[i][0]]\n",
    "\n",
    "    res = []\n",
    "    if using_heap == False:\n",
    "        doc_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        if len(doc_score) > k:\n",
    "            for i in range (k):\n",
    "                res.append(doc_score[i])\n",
    "        else:\n",
    "            res = doc_score\n",
    "    if using_heap == True:\n",
    "        hp = []\n",
    "        for e in doc_score:\n",
    "            hp.append((e[1], e[0]))\n",
    "\n",
    "        nlargest = heapq.nlargest(k, hp)\n",
    "        k_high_ranked = []\n",
    "        for i in nlargest:\n",
    "            k_high_ranked.append([i[1], i[0]])\n",
    "\n",
    "        res = k_high_ranked\n",
    "\n",
    "    docs_id = pd.DataFrame(res, columns=['id', 'rank'])\n",
    "    res_df = print_res(df, docs_id)\n",
    "    return res_df\n",
    "\n",
    "#######################################phase3 Clustering\n",
    "def k_means(k, iteration):\n",
    "    #select k centroids randomly\n",
    "    centroids = []\n",
    "    randoms = np.random.randint(1, len(inverted_index_perDoc) , size=(k))\n",
    "    for i in range(len(randoms)):\n",
    "        centroids.append(inverted_index_perDoc[randoms[i]-1][1])\n",
    "    # print(centroids)\n",
    "\n",
    "    max_index_row = 0\n",
    "    cluster_docs = []       \n",
    "    for it in range(iteration):\n",
    "        print(it)\n",
    "        doc_membership = np.zeros((len(df), k))\n",
    "        for c in range(len(centroids)):\n",
    "            print(\"******c= \",c)\n",
    "            for term in centroids[c]:\n",
    "            # for key,value in centroids[c].items():\n",
    "                # print(dict_inverted_indexes)\n",
    "                docs, idf = search_token(dict_inverted_indexes, term[0])\n",
    "                if docs != -1:\n",
    "                    for doc in docs:\n",
    "                        doc_membership[doc[0]-1][c] = doc_membership[doc[0]-1][c] + term[1]*doc[1]\n",
    "        #update centroids\n",
    "\n",
    "        temp_index_max = max_index_row\n",
    "        max_index_row = np.argmax(doc_membership, axis=1)\n",
    "        if np.array_equal(temp_index_max, max_index_row):\n",
    "            print(\"centroids not changed\")\n",
    "            cluster_name = np.argmax(doc_membership, axis=1)\n",
    "            for c in range(len(centroids)):\n",
    "                cluster_docs.append(np.where(cluster_name == c))\n",
    "\n",
    "            return centroids, doc_membership, cluster_docs\n",
    "\n",
    "        centroids = []\n",
    "        for i in range(k):\n",
    "            # print(\"******k= \",i)\n",
    "            doc_id_perCluster = np.where(max_index_row == i)\n",
    "            # print(doc_id_perCluster)\n",
    "            temp = []\n",
    "            for doc_id in doc_id_perCluster[0]:\n",
    "                # print(doc_id)\n",
    "                # print(\"hey\")\n",
    "                for term in inverted_index_perDoc[doc_id][1]:\n",
    "                    temp.append(term)\n",
    "            centroids.append(temp)\n",
    "        for i in range(k):\n",
    "            # data_items = centroids[i].items()\n",
    "            # data_list = list(data_items)\n",
    "            df_centroid = pd.DataFrame(centroids[i], columns=['term','weight'])\n",
    "            df_centroid = df_centroid.groupby('term', as_index=False)['weight'].mean()\n",
    "            centroids[i] = df_centroid.to_numpy()\n",
    "            # centroids[i] = dict(centroids[i]) \n",
    "\n",
    "        print(doc_membership)\n",
    "        cluster_name = np.argmax(doc_membership, axis=1)\n",
    "        for c in range(len(centroids)):\n",
    "            cluster_docs.append(np.where(cluster_name == c))\n",
    "\n",
    "    return centroids, doc_membership, cluster_docs\n",
    "\n",
    "def ranked_search_withClustering(df, inverted_indexes, query, doc_length, k, using_heap, b2):\n",
    "    res = []\n",
    "    docs_id = pd.DataFrame()\n",
    "    \n",
    "    query = delete_punctuations(query)\n",
    "    query = delete_stopWords(query)\n",
    "    query = delete_postfixes(query)\n",
    "    query = delete_prefixes(query)\n",
    "    query = replaceArabicWords(query)\n",
    "\n",
    "    tokenized_query = query.split()\n",
    "    tokenized_query = replaceWithRoot(tokenized_query)\n",
    "    tokenized_query = pluralToSingular(tokenized_query)\n",
    "\n",
    "    if len(tokenized_query) == 0:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return -1\n",
    "\n",
    "    (unique, counts) = np.unique(tokenized_query, return_counts=True)\n",
    "\n",
    "    tokenized_query = np.asarray((unique, counts)).T\n",
    "    # print(tokenized_query)\n",
    "    doc_score_temp = []\n",
    "    docs_id = np.array\n",
    "    for i in range (0 ,len(tokenized_query)):\n",
    "        tf_query = 1 + math.log(int(tokenized_query[i][1]), 10)\n",
    "        # print(tokenized_query[i][0])\n",
    "        docs, idf = search_token(inverted_indexes, tokenized_query[i][0])\n",
    "        similarityToCentroids = np.zeros(len(centroids))\n",
    "        w_termInQuery = tf_query*idf\n",
    "        for c in range(len(centroids)):\n",
    "            if tokenized_query[i][0] in centroids[c]:\n",
    "                similarityToCentroids[c] = similarityToCentroids[c] + w_termInQuery*centroids[c][tokenized_query[i][0]]\n",
    "    for b in range(b2):\n",
    "        # print(similarityToCentroids)\n",
    "        mostSimilar = np.argmax(similarityToCentroids)\n",
    "        # print(mostSimilar)\n",
    "        similarityToCentroids[mostSimilar] = 0\n",
    "\n",
    "        if b == 0:\n",
    "            docs_id = cluster_docs[mostSimilar][0]\n",
    "        else:\n",
    "            docs_id_temp = cluster_docs[mostSimilar][0]\n",
    "            docs_id = np.append(docs_id, docs_id_temp)\n",
    "\n",
    "    # print(\"*****\")\n",
    "    # print(docs_id)\n",
    "    # print(len(docs_id))\n",
    "\n",
    "    # print(similarityToCentroids)\n",
    "    #search in docs\n",
    "    doc_score_temp = []\n",
    "    doc_score = []\n",
    "    for i in range (0 ,len(tokenized_query)):\n",
    "        # print(tokenized_query[i][0])\n",
    "        tf_query = 1 + math.log(int(tokenized_query[i][1]), 10)\n",
    "        # print(tokenized_query[i][0])\n",
    "        docs, idf = search_token(inverted_indexes, tokenized_query[i][0])\n",
    "\n",
    "        w_termInQuery = tf_query*idf\n",
    "        # print(len(docs_id))\n",
    "        for doc_id in docs_id:\n",
    "            if tokenized_query[i][0] in inverted_index_perDoc[doc_id][1]:\n",
    "                # print(doc_id)\n",
    "                # print(w_termInQuery)\n",
    "                # print(inverted_index_perDoc[doc_id][1][tokenized_query[i][0]])\n",
    "                doc_score_temp.append([doc_id + 1, inverted_index_perDoc[doc_id][1][tokenized_query[i][0]] * w_termInQuery])\n",
    "    if len(doc_score_temp) < 1:\n",
    "        print(\"It looks like there aren't many great matches for your search\")\n",
    "        return -1\n",
    "\n",
    "    doc_score_temp.sort()\n",
    "    # print(doc_score_temp)\n",
    "\n",
    "    doc_score.append(doc_score_temp[0])\n",
    "    # print(len(doc_score_temp))\n",
    "    for i in range(1, len(doc_score_temp)):\n",
    "        if doc_score[len(doc_score)-1][0] == doc_score_temp[i][0]:\n",
    "            # print(doc_score_temp[i][0])\n",
    "            # print(doc_score[len(doc_score)-1][1])\n",
    "            doc_score[len(doc_score)-1][1] = doc_score[len(doc_score)-1][1] + doc_score_temp[i][1]\n",
    "            # print(doc_score[len(doc_score)-1][1])\n",
    "        else:\n",
    "            doc_score.append(doc_score_temp[i])\n",
    "\n",
    "\n",
    "    # for i in range(len(doc_score)):\n",
    "    #     doc_score[i][1] = doc_score[i][1] / doc_length[doc_score[i][0]]\n",
    "\n",
    "    res = []\n",
    "    if using_heap == False:\n",
    "        doc_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        if len(doc_score) > k:\n",
    "            for i in range (k):\n",
    "                res.append(doc_score[i])\n",
    "        else:\n",
    "            res = doc_score\n",
    "    if using_heap == True:\n",
    "        hp = []\n",
    "        for e in doc_score:\n",
    "            hp.append((e[1], e[0]))\n",
    "\n",
    "        nlargest = heapq.nlargest(k, hp)\n",
    "        k_high_ranked = []\n",
    "        for i in nlargest:\n",
    "            k_high_ranked.append([i[1], i[0]])\n",
    "\n",
    "        res = k_high_ranked\n",
    "\n",
    "\n",
    "    docs_id = pd.DataFrame(res, columns=['id', 'rank'])\n",
    "    df_res = print_res(df, docs_id)\n",
    "    return df_res\n"
   ]
  },
  {
   "source": [
    "## Reading datasets and files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_dataset(\"datasets/\", \"IR_Spring2021_ph12_7k.xlsx\")\n",
    "read_files()"
   ]
  },
  {
   "source": [
    "## Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(df)"
   ]
  },
  {
   "source": [
    "## create inverted index per term + champion list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_indexes = create_inverted_indexes(tokens)\r\n",
    "inverted_indexes.pop(0)\r\n",
    "inverted_indexes, doc_length = calculate_tfidf(df, inverted_indexes)\r\n",
    "#normalize inverted_index\r\n",
    "for i in range(len(inverted_indexes)):\r\n",
    "    for j in range(len(inverted_indexes[i][1])):\r\n",
    "        inverted_indexes[i][1][j][1] = inverted_indexes[i][1][j][1] / doc_length[inverted_indexes[i][1][j][0]]\r\n",
    "\r\n",
    "championList = create_championList(inverted_indexes, 4)\r\n",
    "# print(inverted_indexes[1])\r\n",
    "\r\n",
    "##create dictionaries\r\n",
    "dict_inverted_indexes = []\r\n",
    "for i in range(len(inverted_indexes)):\r\n",
    "    temp = []\r\n",
    "    temp.append(inverted_indexes[i][0])\r\n",
    "    temp.append([inverted_indexes[i][1], inverted_indexes[i][2]])\r\n",
    "    dict_inverted_indexes.append(temp)\r\n",
    "# inverted_indexes = dict(inverted_indexes)\r\n",
    "# print(inverted_indexes['سلام'])\r\n",
    "dict_championList = []\r\n",
    "for i in range(len(championList)):\r\n",
    "    temp = []\r\n",
    "    temp.append(championList[i][0])\r\n",
    "    temp.append([championList[i][1], championList[i][2]])\r\n",
    "    dict_championList.append(temp)\r\n",
    "\r\n",
    "dict_inverted_indexes = dict(dict_inverted_indexes)\r\n",
    "dict_championList = dict(dict_championList)"
   ]
  },
  {
   "source": [
    "## Create inverted index per doc"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create inverted_index_perDoc\n",
    "doc_term = []\n",
    "for i in range(len(inverted_indexes)):\n",
    "    for j in range(len(inverted_indexes[i][1])):\n",
    "        doc_term.append([inverted_indexes[i][1][j][0], inverted_indexes[i][0], inverted_indexes[i][1][j][1]])\n",
    "\n",
    "doc_term.sort()\n",
    "# print(doc_term)\n",
    "inverted_index_perDoc = []\n",
    "temp = 0\n",
    "temp = doc_term[0][0]\n",
    "temp1 = []\n",
    "for i in range(0, len(doc_term)):\n",
    "    # print(inverted_index_perDoc[len(inverted_index_perDoc)][0])\n",
    "    if temp == doc_term[i][0]:\n",
    "        temp1.append([doc_term[i][1], doc_term[i][2]])\n",
    "    else:\n",
    "        inverted_index_perDoc.append([temp, temp1])\n",
    "        temp = []\n",
    "        temp1 = []\n",
    "\n",
    "        temp = doc_term[i][0]\n",
    "        temp1.append([doc_term[i][1], doc_term[i][2]])\n",
    "inverted_index_perDoc.append([temp, temp1])"
   ]
  },
  {
   "source": [
    "## K-mean clustering\n",
    "clustering with 5 cluster and 15 iterate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.01635833 0.02583013 0.02802454 0.01622359 0.01485935]\n",
      " [0.01159337 0.02286786 0.03645791 0.00437282 0.01992944]\n",
      " [0.04654571 0.02357023 0.04232842 0.04915135 0.04236345]\n",
      " ...\n",
      " [0.01669898 0.02484786 0.01179503 0.01239389 0.0133338 ]\n",
      " [0.0333959  0.0074656  0.00885971 0.04210428 0.01265786]\n",
      " [0.01553063 0.00414809 0.00892014 0.049225   0.00836284]]\n",
      "1\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.88035023 0.69198294 1.03850731 0.6418104  0.78024407]\n",
      " [0.35471667 0.37662604 0.77069784 0.36872201 0.38244441]\n",
      " [0.85173088 0.82224719 0.85716869 0.79972436 0.70059218]\n",
      " ...\n",
      " [0.99424323 1.24051999 0.86913889 0.74935537 0.74527044]\n",
      " [0.56814908 0.52126303 0.51272382 0.81038211 0.4904896 ]\n",
      " [0.62440987 0.51963553 0.48594854 0.71332696 0.51860228]]\n",
      "2\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.84113325 0.70788066 1.04340338 0.59689236 0.80965285]\n",
      " [0.36211233 0.37133181 0.77006517 0.34196482 0.40417511]\n",
      " [0.80400301 0.83844831 0.92106841 0.61530901 0.72656924]\n",
      " ...\n",
      " [0.97193718 1.25735747 0.86035644 0.73246534 0.75302122]\n",
      " [0.53246148 0.53363319 0.5165028  0.82836371 0.50670292]\n",
      " [0.61116631 0.52862286 0.51122655 0.72907053 0.54173564]]\n",
      "3\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.83619791 0.70404543 1.04316513 0.59722589 0.81826595]\n",
      " [0.36021471 0.37099132 0.75669801 0.34382918 0.40039877]\n",
      " [0.79949743 0.83838301 0.91511888 0.61593661 0.73580023]\n",
      " ...\n",
      " [0.95769318 1.25753766 0.86795976 0.73309708 0.77244315]\n",
      " [0.52665321 0.53343521 0.51431524 0.82873412 0.54196879]\n",
      " [0.60635215 0.52416524 0.50281577 0.7291016  0.5748234 ]]\n",
      "4\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.8341222  0.70316993 1.04242152 0.59725407 0.82136413]\n",
      " [0.35928717 0.37076605 0.75045798 0.34402155 0.38920532]\n",
      " [0.7984286  0.83693448 0.91212598 0.61598368 0.72926381]\n",
      " ...\n",
      " [0.95906803 1.25681547 0.8668664  0.73237354 0.78913129]\n",
      " [0.52508513 0.53285207 0.52004646 0.82866179 0.54544739]\n",
      " [0.60278588 0.52396042 0.51108741 0.7290383  0.60675043]]\n",
      "5\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.83875412 0.70196989 1.03753988 0.59725407 0.82116002]\n",
      " [0.35936783 0.37072368 0.74679513 0.34402155 0.38894928]\n",
      " [0.79910575 0.8364026  0.90928555 0.61598368 0.73080441]\n",
      " ...\n",
      " [0.96162085 1.25668785 0.85660098 0.73237354 0.78846362]\n",
      " [0.52552807 0.53281817 0.51626058 0.82866179 0.5456783 ]\n",
      " [0.60626789 0.52392723 0.50934039 0.7290383  0.60721567]]\n",
      "6\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.83790513 0.70239846 1.03721116 0.59723606 0.82273217]\n",
      " [0.35955527 0.37051181 0.74658994 0.34396527 0.38652987]\n",
      " [0.79900405 0.83641639 0.90889513 0.61571852 0.73258933]\n",
      " ...\n",
      " [0.9613691  1.25673103 0.85648941 0.73222957 0.78830774]\n",
      " [0.52527035 0.53305737 0.51623631 0.82864243 0.54500894]\n",
      " [0.60613837 0.52457405 0.50921913 0.72895034 0.60672468]]\n",
      "7\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.837503   0.70239846 1.03720681 0.59723606 0.82351726]\n",
      " [0.35952281 0.37051181 0.7466002  0.34396527 0.3864208 ]\n",
      " [0.79896925 0.83641639 0.90891646 0.61571852 0.73166773]\n",
      " ...\n",
      " [0.96093533 1.25673103 0.85646989 0.73222957 0.78881185]\n",
      " [0.52473845 0.53305737 0.51622169 0.82864243 0.54465672]\n",
      " [0.6060831  0.52457405 0.50917871 0.72895034 0.6070605 ]]\n",
      "8\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.83734002 0.70235158 1.03761294 0.59723606 0.82584906]\n",
      " [0.35920086 0.37045694 0.74691594 0.34396527 0.38548818]\n",
      " [0.7986601  0.8361688  0.90971004 0.61571852 0.72599553]\n",
      " ...\n",
      " [0.96034472 1.25638027 0.85675939 0.73222957 0.78866338]\n",
      " [0.52446592 0.53303711 0.51622568 0.82864243 0.54485661]\n",
      " [0.60523466 0.52449851 0.50941208 0.72895034 0.60815968]]\n",
      "9\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.83812426 0.70025883 1.0367578  0.59723606 0.82365693]\n",
      " [0.36038415 0.37014187 0.74684886 0.34396527 0.38169926]\n",
      " [0.80023001 0.83219373 0.90959127 0.61571852 0.72247973]\n",
      " ...\n",
      " [0.96072133 1.25502863 0.85647823 0.73222957 0.78674747]\n",
      " [0.52510378 0.53073637 0.51624061 0.82864243 0.54409341]\n",
      " [0.60543127 0.52336626 0.509373   0.72895034 0.60659893]]\n",
      "10\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.83795507 0.70025883 1.0367578  0.59723606 0.82394985]\n",
      " [0.36029712 0.37014187 0.74684886 0.34396527 0.38201954]\n",
      " [0.8000438  0.83219373 0.90959127 0.61571852 0.72295745]\n",
      " ...\n",
      " [0.96056791 1.25502863 0.85647823 0.73222957 0.78714116]\n",
      " [0.52499787 0.53073637 0.51624061 0.82864243 0.5450161 ]\n",
      " [0.60529663 0.52336626 0.509373   0.72895034 0.60710539]]\n",
      "11\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.83782894 0.70025883 1.03668322 0.59723606 0.82452064]\n",
      " [0.35987105 0.37014187 0.74670073 0.34396527 0.38266862]\n",
      " [0.79925469 0.83219373 0.90952894 0.61571852 0.72604255]\n",
      " ...\n",
      " [0.9603122  1.25502863 0.8564541  0.73222957 0.78819825]\n",
      " [0.52454107 0.53073637 0.51623946 0.82864243 0.5469786 ]\n",
      " [0.60502849 0.52336626 0.50937192 0.72895034 0.60763627]]\n",
      "12\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.83719289 0.69932311 1.03668322 0.59723606 0.82535048]\n",
      " [0.35850731 0.36962176 0.74670073 0.34396527 0.38457691]\n",
      " [0.79844156 0.83098886 0.90952894 0.61571852 0.72774917]\n",
      " ...\n",
      " [0.96576528 1.25490316 0.8564541  0.73222957 0.81045362]\n",
      " [0.52313815 0.53038378 0.51623946 0.82864243 0.55037687]\n",
      " [0.60502488 0.52310169 0.50937192 0.72895034 0.606833  ]]\n",
      "13\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.83664449 0.6992957  1.03637782 0.59750278 0.81746341]\n",
      " [0.35479847 0.36956068 0.74638005 0.34404704 0.38718833]\n",
      " [0.79574011 0.83091403 0.90865681 0.61573794 0.76134762]\n",
      " ...\n",
      " [0.96426081 1.25460841 0.85609673 0.73221642 0.81366268]\n",
      " [0.52185876 0.53029954 0.51616906 0.8285756  0.54956583]\n",
      " [0.60353286 0.52306486 0.50932801 0.72870639 0.63263472]]\n",
      "14\n",
      "******c=  0\n",
      "******c=  1\n",
      "******c=  2\n",
      "******c=  3\n",
      "******c=  4\n",
      "[[0.83516382 0.6990089  1.03646053 0.58334994 0.8333959 ]\n",
      " [0.353869   0.36955436 0.74641657 0.34405352 0.38554493]\n",
      " [0.79564791 0.83041071 0.90860593 0.61463313 0.75882708]\n",
      " ...\n",
      " [0.95996653 1.25403307 0.8560517  0.72058642 0.82114127]\n",
      " [0.52178909 0.53011333 0.51610388 0.82860823 0.54011899]\n",
      " [0.60183174 0.52303009 0.50934498 0.72860253 0.62148114]]\n"
     ]
    }
   ],
   "source": [
    "centroids, doc_membership, cluster_docs = k_means(5, 15)\n",
    "# convert to dictionary\n",
    "for i in range(len(inverted_index_perDoc)):\n",
    "    inverted_index_perDoc[i][1] = dict(inverted_index_perDoc[i][1])\n",
    "\n",
    "for i in range(len(centroids)):\n",
    "    centroids[i] = dict(centroids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "binary search\n        id  rank                                                url\n793   6385     3  https://www.isna.ir/news/99111208510/از-آندومت...\n351   5535     3  https://www.isna.ir/news/99020906382/کرونا-از-...\n307   5453     3  https://www.isna.ir/news/99011407171/تولد-دهمی...\n606   6020     3  https://www.isna.ir/news/99062720787/افتتاح-بی...\n567   5946     3  https://www.isna.ir/news/99061107994/۳-اصل-مهم...\n...    ...   ...                                                ...\n357   5548     1  https://www.isna.ir/news/99021410284/بهترین-اف...\n358   5549     1  https://www.isna.ir/news/99021410064/کرونا-پشت...\n359   5550     1  https://www.isna.ir/news/99021410020/شناسایی-۹...\n360   5554     1  https://www.isna.ir/news/99021510817/۱۲۲۳-ابتل...\n1045  7000     1  https://www.isna.ir/news/98092015327/انواع-دیا...\n\n[1046 rows x 3 columns]\nrank search using all posting lists\n     id      rank                                                url\n2  5453  0.699405  https://www.isna.ir/news/99011407171/تولد-دهمی...\n3  5461  0.699405  https://www.isna.ir/news/99011407171/تولد-دهمی...\n8  6886  0.368826  https://www.isna.ir/news/98071813768/خارج-کردن...\n9  6889  0.368826  https://www.isna.ir/news/98071813768/خارج-کردن...\n5  5946  0.353189  https://www.isna.ir/news/99061107994/۳-اصل-مهم...\n6  6013  0.351518  https://www.isna.ir/news/99062720787/افتتاح-بی...\n7  6020  0.351518  https://www.isna.ir/news/99062720787/افتتاح-بی...\n1  4366  0.344058  https://www.isna.ir/news/98012912873/ایران-بهش...\n0  1606  0.334954  https://www.isna.ir/news/98111309170/سفر-تیم-م...\n4  5535  0.331943  https://www.isna.ir/news/99020906382/کرونا-از-...\nrank search using champion list\n     id      rank                                                url\n4  5453  0.452051  https://www.isna.ir/news/99011407171/تولد-دهمی...\n5  5461  0.452051  https://www.isna.ir/news/99011407171/تولد-دهمی...\n1  1606  0.334954  https://www.isna.ir/news/98111309170/سفر-تیم-م...\n2  1921  0.292579  https://www.isna.ir/news/99050100921/جلسه-شورا...\n6  5904  0.275321  https://www.isna.ir/news/99052719974/زن-جوان-ف...\n7  6606  0.256339  https://www.isna.ir/news/98012510503/تولد-چهار...\n3  2913  0.254779  https://www.isna.ir/news/98071511860/انتخاب-اع...\n0   756  0.248147  https://www.isna.ir/news/99091814045/تیروکمان-...\n8  6626  0.120213  https://www.isna.ir/news/98022110734/وزیر-بهدا...\n9  6629  0.120213  https://www.isna.ir/news/98022110734/وزیر-بهدا...\nrank search using clustering\n     id      rank                                                url\n8  6886  0.368826  https://www.isna.ir/news/98071813768/خارج-کردن...\n9  6889  0.368826  https://www.isna.ir/news/98071813768/خارج-کردن...\n5  1606  0.334954  https://www.isna.ir/news/98111309170/سفر-تیم-م...\n7  1921  0.292579  https://www.isna.ir/news/99050100921/جلسه-شورا...\n6  1698  0.229151  https://www.isna.ir/news/98121713481/لیگ-تکوان...\n4   972  0.225456  https://www.isna.ir/news/99120402801/سومی-فولا...\n0   597  0.208519  https://www.isna.ir/news/99072820710/برتری-پرس...\n3   819  0.203495  https://www.isna.ir/news/99101612434/مس-رفسنجا...\n1   600  0.197149  https://www.isna.ir/news/99072921619/شکست-ذوب-...\n2   607  0.196446  https://www.isna.ir/news/99080100388/یک-پیروزی...\nrank search with classification.\n categories: 'sport', 'politics', 'economy', 'health', 'culture'\nhealth\n     id      rank                                                url\n0  5453  0.699405  https://www.isna.ir/news/99011407171/تولد-دهمی...\n1  5461  0.699405  https://www.isna.ir/news/99011407171/تولد-دهمی...\n8  6886  0.368826  https://www.isna.ir/news/98071813768/خارج-کردن...\n9  6889  0.368826  https://www.isna.ir/news/98071813768/خارج-کردن...\n4  5946  0.353189  https://www.isna.ir/news/99061107994/۳-اصل-مهم...\n5  6013  0.351518  https://www.isna.ir/news/99062720787/افتتاح-بی...\n6  6020  0.351518  https://www.isna.ir/news/99062720787/افتتاح-بی...\n2  5535  0.331943  https://www.isna.ir/news/99020906382/کرونا-از-...\n3  5738  0.316212  https://www.isna.ir/news/99041410234/چگونگی-تش...\n7  6681  0.311661  https://www.isna.ir/news/98032712929/استفاده-ا...\n"
     ]
    }
   ],
   "source": [
    "query = \"دکتر زنان و زایمان\"\n",
    "\n",
    "# binary search\n",
    "print(\"binary search\")\n",
    "print(binary_search(df, dict_inverted_indexes, query))\n",
    "# rank search using all posting lists\n",
    "print(\"rank search using all posting lists\")\n",
    "print(ranked_search(df, dict_inverted_indexes, query, doc_length, k = 10, using_heap = True))\n",
    "# rank search using champion list\n",
    "print(\"rank search using champion list\")\n",
    "print(ranked_search(df, dict_championList, query, doc_length, k = 10, using_heap = True))\n",
    "# rank search using clustering to pure some unrelated docs\n",
    "print(\"rank search using clustering\")\n",
    "print(ranked_search_withClustering(df, dict_inverted_indexes, query, doc_length, k = 10, using_heap = True, b2 = 2))\n",
    "\n",
    "query = \"cat:health دکتر زنان و زایمان\"\n",
    "print(\"rank search with classification.\\n categories: 'sport', 'politics', 'economy', 'health', 'culture'\")\n",
    "print(ranked_search(df, dict_inverted_indexes, query, doc_length, k = 10, using_heap = True))\n"
   ]
  },
  {
   "source": [
    "## Labeling datasets Using KNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_11k = read_dataset(\"datasets/\", \"IR00_3_11k News.xlsx\")\n",
    "df_17k = read_dataset(\"datasets/\", \"IR00_3_17k News.xlsx\")\n",
    "df_20k = read_dataset(\"datasets/\", \"IR00_3_20k News.xlsx\")\n",
    "df_50k = pd.concat([df_11k, df_17k, df_20k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50k['id']  = range(1, 1 + len(df_50k))\n",
    "# duplicates_loaded = len(df_50k[df_50k.duplicated(subset=['url', 'content', 'topic'])])\n",
    "# print(duplicates_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50k = df_50k.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_50k = tokenize(df_50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16898010"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "len(tokens_50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_indexes_50k = create_inverted_indexes(tokens_50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "243026"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "inverted_indexes_50k.pop(0)\n",
    "inverted_indexes_50k, doc_length_50k = calculate_tfidf(df_50k, inverted_indexes_50k)\n",
    "#normalize inverted_index\n",
    "for i in range(len(inverted_indexes_50k)):\n",
    "    for j in range(len(inverted_indexes_50k[i][1])):\n",
    "        inverted_indexes_50k[i][1][j][1] = inverted_indexes_50k[i][1][j][1] / doc_length_50k[inverted_indexes_50k[i][1][j][0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_inverted_indexes_50k = []\n",
    "for i in range(len(inverted_indexes_50k)):\n",
    "    temp = []\n",
    "    temp.append(inverted_indexes_50k[i][0])\n",
    "    temp.append([inverted_indexes_50k[i][1], inverted_indexes_50k[i][2]])\n",
    "    dict_inverted_indexes_50k.append(temp)\n",
    "# inverted_indexes = dict(inverted_indexes)\n",
    "# print(inverted_indexes['سلام'])\n",
    "# dict_championList = []\n",
    "# for i in range(len(championList)):\n",
    "#     temp = []\n",
    "#     temp.append(championList[i][0])\n",
    "#     temp.append([championList[i][1], championList[i][2]])\n",
    "#     dict_championList.append(temp)\n",
    "\n",
    "dict_inverted_indexes_50k = dict(dict_inverted_indexes_50k)\n",
    "# dict_championList = dict(dict_championList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['sport', 'politics', 'economy', 'health', 'culture'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "# df_50k.loc[df_50k['topic'] == 'sports'] = df_50k.loc[df_50k['topic'] == 'sports'].topic.replace('sport')\n",
    "df_50k['topic'] = df_50k['topic'].replace('sports','sport')\n",
    "df_50k['topic'] = df_50k['topic'].replace('political','politics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          id                                            content     topic  \\\n",
       "0          1  به گزارش ایسنا، پس از استعفا علی نکویی رئیس شا...     sport   \n",
       "1          2  به گزارش ایسنا، امیر محسنی با بیان اینکه این ل...     sport   \n",
       "2          3  آرش فرهادیان در گفت و گو با ایسنا، درباره آخری...     sport   \n",
       "3          4  به گزارش ایسنا، فدراسیون بین المللی شنا قانونی...     sport   \n",
       "4          5  به گزارش ایسنا، فدراسیون جهانی صعودهای ورزشی (...     sport   \n",
       "...      ...                                                ...       ...   \n",
       "50056  50057  به گزارش حوزه پارلمانی خبرگزاری فارس، [محمدباق...  politics   \n",
       "50057  50058  به گزارش خبرنگار تشکل‌های دانشگاهی خبرگزاری فا...  politics   \n",
       "50058  50059  به گزارش خبرنگار حوزه دولت خبرگزاری فارس، رضا ...  politics   \n",
       "50059  50060  به گزارش خبرنگار دولت خبرگزاری فارس، محسن حاجی...  politics   \n",
       "50060  50061  حجت‌الاسلام علیرضا سلیمی نماینده مردم محلات در...  politics   \n",
       "\n",
       "                                                     url  \n",
       "0      https://www.isna.ir/news/99010100077/حواشی-در-...  \n",
       "1      https://www.isna.ir/news/98122922468/ثبت-نام-ب...  \n",
       "2      https://www.isna.ir/news/99010200541/فرهادیان-...  \n",
       "3      https://www.isna.ir/news/99010200528/فناوری-وی...  \n",
       "4      https://www.isna.ir/news/99010200510/تعویق-زما...  \n",
       "...                                                  ...  \n",
       "50056  https://www.farsnews.ir/news/13990612000494/پی...  \n",
       "50057  https://www.farsnews.ir/news/13990612000397/اع...  \n",
       "50058  https://www.farsnews.ir/news/13990612000425/اف...  \n",
       "50059  https://www.farsnews.ir/news/13990612000435/وز...  \n",
       "50060  https://www.farsnews.ir/news/13990611001190/سل...  \n",
       "\n",
       "[50061 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>content</th>\n      <th>topic</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>به گزارش ایسنا، پس از استعفا علی نکویی رئیس شا...</td>\n      <td>sport</td>\n      <td>https://www.isna.ir/news/99010100077/حواشی-در-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>به گزارش ایسنا، امیر محسنی با بیان اینکه این ل...</td>\n      <td>sport</td>\n      <td>https://www.isna.ir/news/98122922468/ثبت-نام-ب...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>آرش فرهادیان در گفت و گو با ایسنا، درباره آخری...</td>\n      <td>sport</td>\n      <td>https://www.isna.ir/news/99010200541/فرهادیان-...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>به گزارش ایسنا، فدراسیون بین المللی شنا قانونی...</td>\n      <td>sport</td>\n      <td>https://www.isna.ir/news/99010200528/فناوری-وی...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>به گزارش ایسنا، فدراسیون جهانی صعودهای ورزشی (...</td>\n      <td>sport</td>\n      <td>https://www.isna.ir/news/99010200510/تعویق-زما...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>50056</th>\n      <td>50057</td>\n      <td>به گزارش حوزه پارلمانی خبرگزاری فارس، [محمدباق...</td>\n      <td>politics</td>\n      <td>https://www.farsnews.ir/news/13990612000494/پی...</td>\n    </tr>\n    <tr>\n      <th>50057</th>\n      <td>50058</td>\n      <td>به گزارش خبرنگار تشکل‌های دانشگاهی خبرگزاری فا...</td>\n      <td>politics</td>\n      <td>https://www.farsnews.ir/news/13990612000397/اع...</td>\n    </tr>\n    <tr>\n      <th>50058</th>\n      <td>50059</td>\n      <td>به گزارش خبرنگار حوزه دولت خبرگزاری فارس، رضا ...</td>\n      <td>politics</td>\n      <td>https://www.farsnews.ir/news/13990612000425/اف...</td>\n    </tr>\n    <tr>\n      <th>50059</th>\n      <td>50060</td>\n      <td>به گزارش خبرنگار دولت خبرگزاری فارس، محسن حاجی...</td>\n      <td>politics</td>\n      <td>https://www.farsnews.ir/news/13990612000435/وز...</td>\n    </tr>\n    <tr>\n      <th>50060</th>\n      <td>50061</td>\n      <td>حجت‌الاسلام علیرضا سلیمی نماینده مردم محلات در...</td>\n      <td>politics</td>\n      <td>https://www.farsnews.ir/news/13990611001190/سل...</td>\n    </tr>\n  </tbody>\n</table>\n<p>50061 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "len(dict_inverted_indexes_50k)"
   ]
  },
  {
   "source": [
    "## KNN\n",
    "label our unlabeled datasets using another datasets labels using KNN classification algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "Wall time: 18min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_topics = {}\n",
    "for i in range(len(df):\n",
    "    query = df.content[i]\n",
    "    # print(query)\n",
    "    res = ranked_search(df_50k, dict_inverted_indexes_50k, query, doc_length_50k, k = 9, using_heap = True)\n",
    "    className = res['topic'].value_counts().idxmax()\n",
    "    dict_topics[i+1] = className\n",
    "    if i%1000 == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'economy'"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "dict_topics[2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'files/'\n",
    "a_file = open(path + \"IR_Spring2021_ph12_7k_lablesDict.pkl\", \"wb\")\n",
    "pickle.dump(dict_topics, a_file)\n",
    "a_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'files/'\n",
    "a_file = open(path + \"IR_Spring2021_ph12_7k_lablesDict.pkl\", \"rb\")\n",
    "dict_topics = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'sport'"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "dict_topics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates_loaded = len(df_50k[df_50k.duplicated(subset=['url'])])\n",
    "# print(duplicates_loaded)\n"
   ]
  },
  {
   "source": [
    "## Queries and results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "binary search\n        id  rank                                                url\n793   6385     3  https://www.isna.ir/news/99111208510/از-آندومت...\n351   5535     3  https://www.isna.ir/news/99020906382/کرونا-از-...\n307   5453     3  https://www.isna.ir/news/99011407171/تولد-دهمی...\n606   6020     3  https://www.isna.ir/news/99062720787/افتتاح-بی...\n567   5946     3  https://www.isna.ir/news/99061107994/۳-اصل-مهم...\n...    ...   ...                                                ...\n357   5548     1  https://www.isna.ir/news/99021410284/بهترین-اف...\n358   5549     1  https://www.isna.ir/news/99021410064/کرونا-پشت...\n359   5550     1  https://www.isna.ir/news/99021410020/شناسایی-۹...\n360   5554     1  https://www.isna.ir/news/99021510817/۱۲۲۳-ابتل...\n1045  7000     1  https://www.isna.ir/news/98092015327/انواع-دیا...\n\n[1046 rows x 3 columns]\nrank search using all posting lists\n     id      rank                                                url\n2  5453  0.699405  https://www.isna.ir/news/99011407171/تولد-دهمی...\n3  5461  0.699405  https://www.isna.ir/news/99011407171/تولد-دهمی...\n8  6886  0.368826  https://www.isna.ir/news/98071813768/خارج-کردن...\n9  6889  0.368826  https://www.isna.ir/news/98071813768/خارج-کردن...\n5  5946  0.353189  https://www.isna.ir/news/99061107994/۳-اصل-مهم...\n6  6013  0.351518  https://www.isna.ir/news/99062720787/افتتاح-بی...\n7  6020  0.351518  https://www.isna.ir/news/99062720787/افتتاح-بی...\n1  4366  0.344058  https://www.isna.ir/news/98012912873/ایران-بهش...\n0  1606  0.334954  https://www.isna.ir/news/98111309170/سفر-تیم-م...\n4  5535  0.331943  https://www.isna.ir/news/99020906382/کرونا-از-...\nrank search using champion list\n     id      rank                                                url\n4  5453  0.452051  https://www.isna.ir/news/99011407171/تولد-دهمی...\n5  5461  0.452051  https://www.isna.ir/news/99011407171/تولد-دهمی...\n1  1606  0.334954  https://www.isna.ir/news/98111309170/سفر-تیم-م...\n2  1921  0.292579  https://www.isna.ir/news/99050100921/جلسه-شورا...\n6  5904  0.275321  https://www.isna.ir/news/99052719974/زن-جوان-ف...\n7  6606  0.256339  https://www.isna.ir/news/98012510503/تولد-چهار...\n3  2913  0.254779  https://www.isna.ir/news/98071511860/انتخاب-اع...\n0   756  0.248147  https://www.isna.ir/news/99091814045/تیروکمان-...\n8  6626  0.120213  https://www.isna.ir/news/98022110734/وزیر-بهدا...\n9  6629  0.120213  https://www.isna.ir/news/98022110734/وزیر-بهدا...\nrank search using clustering\n     id      rank                                                url\n8  6886  0.368826  https://www.isna.ir/news/98071813768/خارج-کردن...\n9  6889  0.368826  https://www.isna.ir/news/98071813768/خارج-کردن...\n5  1606  0.334954  https://www.isna.ir/news/98111309170/سفر-تیم-م...\n7  1921  0.292579  https://www.isna.ir/news/99050100921/جلسه-شورا...\n6  1698  0.229151  https://www.isna.ir/news/98121713481/لیگ-تکوان...\n4   972  0.225456  https://www.isna.ir/news/99120402801/سومی-فولا...\n0   597  0.208519  https://www.isna.ir/news/99072820710/برتری-پرس...\n3   819  0.203495  https://www.isna.ir/news/99101612434/مس-رفسنجا...\n1   600  0.197149  https://www.isna.ir/news/99072921619/شکست-ذوب-...\n2   607  0.196446  https://www.isna.ir/news/99080100388/یک-پیروزی...\nrank search with classification.\n categories: 'sport', 'politics', 'economy', 'health', 'culture'\nhealth\n     id      rank                                                url\n0  5453  0.699405  https://www.isna.ir/news/99011407171/تولد-دهمی...\n1  5461  0.699405  https://www.isna.ir/news/99011407171/تولد-دهمی...\n8  6886  0.368826  https://www.isna.ir/news/98071813768/خارج-کردن...\n9  6889  0.368826  https://www.isna.ir/news/98071813768/خارج-کردن...\n4  5946  0.353189  https://www.isna.ir/news/99061107994/۳-اصل-مهم...\n5  6013  0.351518  https://www.isna.ir/news/99062720787/افتتاح-بی...\n6  6020  0.351518  https://www.isna.ir/news/99062720787/افتتاح-بی...\n2  5535  0.331943  https://www.isna.ir/news/99020906382/کرونا-از-...\n3  5738  0.316212  https://www.isna.ir/news/99041410234/چگونگی-تش...\n7  6681  0.311661  https://www.isna.ir/news/98032712929/استفاده-ا...\n"
     ]
    }
   ],
   "source": [
    "query = \"دکتر زنان و زایمان\"\n",
    "\n",
    "# binary search\n",
    "print(\"binary search\")\n",
    "print(binary_search(df, dict_inverted_indexes, query))\n",
    "# rank search using all posting lists\n",
    "print(\"rank search using all posting lists\")\n",
    "print(ranked_search(df, dict_inverted_indexes, query, doc_length, k = 10, using_heap = True))\n",
    "# rank search using champion list\n",
    "print(\"rank search using champion list\")\n",
    "print(ranked_search(df, dict_championList, query, doc_length, k = 10, using_heap = True))\n",
    "# rank search using clustering to pure some unrelated docs\n",
    "print(\"rank search using clustering\")\n",
    "print(ranked_search_withClustering(df, dict_inverted_indexes, query, doc_length, k = 10, using_heap = True, b2 = 2))\n",
    "\n",
    "query = \"cat:health دکتر زنان و زایمان\"\n",
    "print(\"rank search with classification.\\n categories: 'sport', 'politics', 'economy', 'health', 'culture'\")\n",
    "print(ranked_search(df, dict_inverted_indexes, query, doc_length, k = 10, using_heap = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}